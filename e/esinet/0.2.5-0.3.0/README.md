# Comparing `tmp/esinet-0.2.5-py3-none-any.whl.zip` & `tmp/esinet-0.3.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,26 +1,26 @@
-Zip file size: 47498 bytes, number of entries: 24
--rw-rw-rw-  2.0 fat       56 b- defN 21-Aug-28 09:00 esinet/__init__.py
--rw-rw-rw-  2.0 fat     3969 b- defN 21-Nov-23 22:58 esinet/custom_layers.py
--rw-rw-rw-  2.0 fat     8434 b- defN 22-Jul-23 15:10 esinet/losses.py
--rw-rw-rw-  2.0 fat    60610 b- defN 22-Jul-24 12:10 esinet/net.py
--rw-rw-rw-  2.0 fat    36168 b- defN 22-Jul-21 19:51 esinet/simulation.py
--rw-rw-rw-  2.0 fat       23 b- defN 22-Jun-22 19:10 esinet/evaluate/__init__.py
--rw-rw-rw-  2.0 fat    18892 b- defN 22-Jul-23 21:09 esinet/evaluate/evaluate.py
--rw-rw-rw-  2.0 fat       22 b- defN 21-Aug-28 09:00 esinet/forward/__init__.py
--rw-rw-rw-  2.0 fat     3028 b- defN 22-Jun-22 19:10 esinet/forward/forward.py
--rw-rw-rw-  2.0 fat       27 b- defN 21-Aug-28 09:00 esinet/minimum_norm/__init__.py
--rw-rw-rw-  2.0 fat     2390 b- defN 22-Jun-22 19:10 esinet/minimum_norm/minimum_norm.py
--rw-rw-rw-  2.0 fat       32 b- defN 21-Aug-28 09:00 esinet/tests/__init__.py
--rw-rw-rw-  2.0 fat     1386 b- defN 22-Jul-24 23:07 esinet/tests/test_net.py
--rw-rw-rw-  2.0 fat     2384 b- defN 22-Jun-22 19:10 esinet/tests/test_simulation.py
--rw-rw-rw-  2.0 fat       19 b- defN 21-Aug-28 09:00 esinet/util/__init__.py
--rw-rw-rw-  2.0 fat    27022 b- defN 22-Jul-21 19:51 esinet/util/util.py
--rw-rw-rw-  2.0 fat       40 b- defN 21-Aug-28 09:00 esinet/viz/__init__.py
--rw-rw-rw-  2.0 fat     2898 b- defN 21-Aug-28 09:00 esinet/viz/cmaps.py
--rw-rw-rw-  2.0 fat       65 b- defN 21-Aug-28 09:00 esinet/viz/viz.py
--rw-rw-rw-  2.0 fat     1088 b- defN 22-Jul-24 23:20 esinet-0.2.5.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     4904 b- defN 22-Jul-24 23:20 esinet-0.2.5.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 22-Jul-24 23:20 esinet-0.2.5.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        7 b- defN 22-Jul-24 23:20 esinet-0.2.5.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     1891 b- defN 22-Jul-24 23:20 esinet-0.2.5.dist-info/RECORD
-24 files, 175447 bytes uncompressed, 44464 bytes compressed:  74.7%
+Zip file size: 48048 bytes, number of entries: 24
+-rw-rw-rw-  2.0 fat       64 b- defN 24-Apr-23 16:21 esinet/__init__.py
+-rw-rw-rw-  2.0 fat     3969 b- defN 21-Aug-24 09:45 esinet/custom_layers.py
+-rw-rw-rw-  2.0 fat    10201 b- defN 24-Apr-23 16:21 esinet/losses.py
+-rw-rw-rw-  2.0 fat    56596 b- defN 24-Apr-25 21:30 esinet/net.py
+-rw-rw-rw-  2.0 fat    39299 b- defN 24-Apr-23 17:03 esinet/simulation.py
+-rw-rw-rw-  2.0 fat       23 b- defN 22-Apr-20 10:04 esinet/evaluate/__init__.py
+-rw-rw-rw-  2.0 fat    19451 b- defN 24-Apr-23 16:21 esinet/evaluate/evaluate.py
+-rw-rw-rw-  2.0 fat       22 b- defN 21-Jul-06 13:08 esinet/forward/__init__.py
+-rw-rw-rw-  2.0 fat     3028 b- defN 22-Apr-20 10:04 esinet/forward/forward.py
+-rw-rw-rw-  2.0 fat       27 b- defN 21-Aug-19 11:07 esinet/minimum_norm/__init__.py
+-rw-rw-rw-  2.0 fat     2390 b- defN 22-Apr-20 10:04 esinet/minimum_norm/minimum_norm.py
+-rw-rw-rw-  2.0 fat       32 b- defN 21-Aug-19 11:07 esinet/tests/__init__.py
+-rw-rw-rw-  2.0 fat     1393 b- defN 24-Apr-23 16:21 esinet/tests/test_net.py
+-rw-rw-rw-  2.0 fat     2378 b- defN 24-Apr-23 16:21 esinet/tests/test_simulation.py
+-rw-rw-rw-  2.0 fat       19 b- defN 21-Jul-06 13:08 esinet/util/__init__.py
+-rw-rw-rw-  2.0 fat    27416 b- defN 24-Apr-23 16:21 esinet/util/util.py
+-rw-rw-rw-  2.0 fat       40 b- defN 21-Jul-06 13:08 esinet/viz/__init__.py
+-rw-rw-rw-  2.0 fat     2898 b- defN 21-Jul-06 13:08 esinet/viz/cmaps.py
+-rw-rw-rw-  2.0 fat       65 b- defN 21-Aug-19 11:07 esinet/viz/viz.py
+-rw-rw-rw-  2.0 fat     1088 b- defN 24-Apr-25 21:45 esinet-0.3.0.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     5027 b- defN 24-Apr-25 21:45 esinet-0.3.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-Apr-25 21:45 esinet-0.3.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        7 b- defN 24-Apr-25 21:45 esinet-0.3.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     1892 b- defN 24-Apr-25 21:45 esinet-0.3.0.dist-info/RECORD
+24 files, 177417 bytes uncompressed, 45014 bytes compressed:  74.6%
```

## zipnote {}

```diff
@@ -51,23 +51,23 @@
 
 Filename: esinet/viz/cmaps.py
 Comment: 
 
 Filename: esinet/viz/viz.py
 Comment: 
 
-Filename: esinet-0.2.5.dist-info/LICENSE
+Filename: esinet-0.3.0.dist-info/LICENSE
 Comment: 
 
-Filename: esinet-0.2.5.dist-info/METADATA
+Filename: esinet-0.3.0.dist-info/METADATA
 Comment: 
 
-Filename: esinet-0.2.5.dist-info/WHEEL
+Filename: esinet-0.3.0.dist-info/WHEEL
 Comment: 
 
-Filename: esinet-0.2.5.dist-info/top_level.txt
+Filename: esinet-0.3.0.dist-info/top_level.txt
 Comment: 
 
-Filename: esinet-0.2.5.dist-info/RECORD
+Filename: esinet-0.3.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## esinet/__init__.py

```diff
@@ -1,2 +1,2 @@
 from .simulation import Simulation
-from .net import Net
+from .net import Net, CovNet
```

## esinet/losses.py

```diff
@@ -248,8 +248,51 @@
         # print(y_true, y_pred)
         batched_losses = tf.map_fn(lambda x:
                                     loss(x[0], x[1]),
                                     (y_true, y_pred), dtype=tf.float32)
         error = K.mean(tf.stack(batched_losses))
         # print("error on all samples and all times: ", error)
         return error
+    return loss_batch
+
+def chamfer2(pos, thresh=0.2, dtype=tf.float32):
+    dist = tf.cast(cdist(pos, pos), dtype=dtype)
+    pos = tf.cast(pos, dtype=dtype)
+    def loss_batch(y_true, y_pred):
+        def loss(y_true, y_pred):
+            # print("third: ", tf.shape(y_true))
+        
+            # print(y_true, y_pred)
+            # find indices above threshold
+            idc_true = tf.where(K.abs(y_true) > K.max(K.abs(y_true)) * thresh)[:, 0]
+            idc_pred = tf.where(K.abs(y_pred) > K.max(K.abs(y_pred)) * thresh)[:, 0]
+            # print(idc_true, idc_pred)
+            # retrieve the correct distances
+            dist_true = tf.gather(dist, idc_true, axis=0)
+            dist_true = tf.gather(dist_true, idc_pred, axis=1)
+            
+            # print(dist_true)
+            
+            
+
+            lowest_dists_1 = tf.reduce_min(dist_true, axis=0)
+            lowest_dists_2 = tf.reduce_min(dist_true, axis=1)
+
+            sum_squares_1 = K.mean(lowest_dists_1)
+            sum_squares_2 = K.mean(lowest_dists_2)
+
+
+            error = (sum_squares_1 + sum_squares_2) / 2
+            # print("error on single sample and time: ", error)
+            return error
+        # reshaping
+        # new_shape = (tf.shape(y_true)[0]*tf.shape(y_true)[1], tf.shape(y_true)[2])
+        # y_true = tf.reshape(y_true, new_shape)
+        # y_pred = tf.reshape(y_pred, new_shape)
+        # print(y_true, y_pred)
+        batched_losses = tf.map_fn(lambda x:
+                                    loss(x[0], x[1]),
+                                    (y_true, y_pred), dtype=tf.float32)
+        error = K.mean(tf.stack(batched_losses))
+        # print("error on all samples and all times: ", error)
+        return error
     return loss_batch
```

## esinet/net.py

```diff
@@ -4,15 +4,15 @@
 from mne.channels.layout import _find_topomap_coords
 import os
 import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import layers
 from tensorflow.keras.layers import (LSTM, GRU, Dense, Flatten, Bidirectional, 
     TimeDistributed, InputLayer, Activation, Reshape, concatenate, Concatenate, 
-    Dropout, Conv2D, multiply)
+    Dropout, Conv1D, Conv2D, multiply)
 from tensorflow.keras import backend as K
 from tensorflow.keras.layers import Lambda
 from tensorflow.keras.preprocessing.sequence import pad_sequences
 # from tensorflow.keras.utils import pad_sequences
 
 from scipy.optimize import minimize_scalar
 # import pickle as pkl
@@ -65,24 +65,25 @@
     train : trains the neural network with the EEG and source data
     predict : perform prediciton on EEG data
     evaluate : evaluate the performance of the model
     '''
     
     def __init__(self, fwd, n_dense_layers=1, n_lstm_layers=2, 
         n_dense_units=200, n_lstm_units=32, activation_function='tanh', 
-        n_filters=8, kernel_size=(3,3), n_jobs=-1, model_type='auto', 
+        n_filters=64, kernel_size=(3,3), l1_reg=None, n_jobs=-1, model_type='auto', 
         scale_individually=True, rescale_sources='brent', 
-        verbose=True):
+        verbose=0):
 
         self._embed_fwd(fwd)
         
         self.n_dense_layers = n_dense_layers
         self.n_lstm_layers = n_lstm_layers
         self.n_dense_units = n_dense_units
         self.n_lstm_units = n_lstm_units
+        self.l1_reg = l1_reg
         self.activation_function = activation_function
         self.n_filters = n_filters
         self.kernel_size = kernel_size
         # self.default_loss = tf.keras.losses.Huber(delta=delta)
         self.default_loss = 'mean_squared_error'  # losses.weighted_huber_loss
         # self.parallel = parallel
         self.n_jobs = n_jobs
@@ -141,15 +142,15 @@
         else:
             msg = f'Input is {type()} must be either the EEG data and Source data or the Simulation object.'
             raise AttributeError(msg)
 
         return eeg, sources
 
     def fit(self, *args, optimizer=None, learning_rate=0.001, 
-        validation_split=0.1, epochs=50, metrics=None, device=None, 
+        validation_split=0.05, epochs=50, metrics=None, device=None, 
         false_positive_penalty=2, delta=1., batch_size=8, loss=None, 
         sample_weight=None, return_history=False, dropout=0.2, patience=7, 
         tensorboard=False, validation_freq=1, revert_order=True):
         ''' Train the neural network using training data (eeg) and labels (sources).
         
         Parameters
         ----------
@@ -211,35 +212,25 @@
             tensorboard_callback = tf.keras.callbacks.TensorBoard(
                 log_dir=log_dir, histogram_freq=1)
             callbacks = [es, tensorboard_callback]
         else:
             callbacks = []#[es]
         if optimizer is None:
             optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
-            # optimizer = tf.keras.optimizers.Adam(clipvalue=0.5)  # clipnorm=1.)
-            # optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate,
-                # momentum=0.35)
         if self.loss is None:
-            # self.loss = self.default_loss(weight=false_positive_penalty, delta=delta)
-            # self.loss = 'mean_squared_error'
             self.loss = tf.keras.losses.CosineSimilarity()
 
 
         elif type(loss) == list:
             self.loss = self.loss[0](*self.loss[1])
-        if metrics is None:
-            # metrics = [self.default_loss(weight=false_positive_penalty, delta=delta)]
-            metrics = ['mae']
         
         # Compile if it wasnt compiled before
         if not self.compiled:
             self.model.compile(optimizer, self.loss, metrics=metrics)
             self.compiled = True
-        # print("shapes before fit: ", x_scaled.shape, y_scaled.shape)
-        
         
         if self.model_type.lower() == 'convdip':
             # print("interpolating for convdip...")
             elec_pos = _find_topomap_coords(self.info, self.info.ch_names)
             interpolator = self.make_interpolator(elec_pos, res=self.interp_channel_shape[0])
             x_scaled_interp = deepcopy(x_scaled)
             for i, sample in enumerate(x_scaled):
@@ -259,69 +250,63 @@
         stop_idx = int(round(n_samples * (1-validation_split)))
         gen = self.generate_batches(x_scaled[:stop_idx], y_scaled[:stop_idx], batch_size, revert_order=revert_order)
         steps_per_epoch = stop_idx // batch_size
         validation_data = (pad_sequences(x_scaled[stop_idx:], dtype='float32'), pad_sequences(y_scaled[stop_idx:], dtype='float32'))
 
         
         if device is None:
-            # history = self.model.fit(x_scaled, y_scaled, 
-            #     epochs=epochs, batch_size=batch_size, shuffle=True, 
-            #     validation_split=validation_split, verbose=self.verbose, 
-            #     callbacks=callbacks, sample_weight=sample_weight)
             history = self.model.fit(x=gen, 
                     epochs=epochs, batch_size=batch_size, 
                     steps_per_epoch=steps_per_epoch, verbose=self.verbose, callbacks=callbacks, 
                     sample_weight=sample_weight, validation_data=validation_data, 
-                    validation_freq=validation_freq, workers=1)
+                    validation_freq=validation_freq)
         else:
             with tf.device(device):
-                # history = self.model.fit(x_scaled, y_scaled, 
-                #     epochs=epochs, batch_size=batch_size, shuffle=True, 
-                #     validation_split=validation_split, verbose=self.verbose,
-                #     callbacks=callbacks, sample_weight=sample_weight)
-                # history = self.model.fit_generator(gen)
                 history = self.model.fit(x=gen, 
                     epochs=epochs, batch_size=batch_size, 
                     steps_per_epoch=steps_per_epoch, verbose=self.verbose, callbacks=callbacks, 
                     sample_weight=sample_weight, validation_data=validation_data, 
-                    validation_freq=validation_freq, workers=1)
+                    validation_freq=validation_freq)
                 
 
         del x_scaled, y_scaled
         if return_history:
             return self, history
         else:
             return self
     @staticmethod
     def generate_batches(x, y, batch_size, revert_order=True):
-            # print('start generator')
             n_batches = int(len(x) / batch_size)
             x = x[:int(n_batches*batch_size)]
             y = y[:int(n_batches*batch_size)]
             
             time_lengths = [x_let.shape[0] for x_let in x]
             idc = list(np.argsort(time_lengths).astype(int))
-            # print("len idc: ", len(idc), " idc: ", idc)
             
             x = [x[i] for i in idc]
             y = [y[i] for i in idc]
             while True:
                 x_pad = []
                 y_pad = []
                 for batch in range(n_batches):
-                    # print(batch, len(x), batch*batch_size, (batch+1)*batch_size, x[batch*batch_size:(batch+1)*batch_size])
                     x_batch = x[batch*batch_size:(batch+1)*batch_size]
                     y_batch = y[batch*batch_size:(batch+1)*batch_size]
+                    
+
                     if revert_order:
                         if np.random.randn()>0:
-                            x_batch = np.flip(x_batch, axis=1)
-                            y_batch = np.flip(y_batch, axis=1)
+                            # x_batch = np.flip(x_batch, axis=1)
+                            # y_batch = np.flip(y_batch, axis=1)
+                            x_batch = [np.flip(xx, axis=1) for xx in x_batch]
+                            y_batch = [np.flip(yy, axis=1) for yy in y_batch]
+                    
+                    
+                    
                     x_padlet = pad_sequences(x_batch , dtype='float32' )
                     y_padlet = pad_sequences(y_batch , dtype='float32' )
-
                     
                         
                     x_pad.append( x_padlet )
                     y_pad.append( y_padlet )
                 
                 new_order = np.arange(len(x_pad))
                 np.random.shuffle(new_order)
@@ -389,15 +374,15 @@
 
         # Ensure that the forward model has the same 
         # channels as the eeg object
         self._check_model(eeg)
 
         # Handle EEG input
         if (type(eeg) == list and isinstance(eeg[0], util.EPOCH_INSTANCES)) or isinstance(eeg, util.EPOCH_INSTANCES):
-            eeg = [eeg[i].get_data() for i, _ in enumerate(eeg)]
+            eeg = [eeg[i].get_data(copy=True) for i, _ in enumerate(eeg)]
         else:
             eeg = [sample_eeg[0] for sample_eeg in eeg]
 
         for i, eeg_sample in enumerate(eeg):
             if len(eeg_sample.shape) == 1:
                 eeg[i] = eeg_sample[:, np.newaxis]
             if len(eeg_sample.shape) == 3:
@@ -448,15 +433,17 @@
         eeg_out = deepcopy(eeg)
         
         if self.scale_individually:
             for sample, eeg_sample in enumerate(eeg):
                 # Common average ref:
                 for time in range(eeg_sample.shape[-1]):
                     eeg_out[sample][:, time] -= np.mean(eeg_sample[:, time])
-                    eeg_out[sample][:, time] /= np.max(np.abs(eeg_sample[:, time]))
+                    # eeg_out[sample][:, time] /= np.max(np.abs(eeg_sample[:, time]))
+                    eeg_out[sample][:, time] /= eeg_out[sample][:, time].std()
+                    
                     
         else:
             for sample, eeg_sample in enumerate(eeg):
                 eeg_out[sample] = self.robust_minmax_scaler(eeg_sample)
                 # Common average ref:
                 for time in range(eeg_sample.shape[-1]):
                     eeg_out[sample][:, time] -= np.mean(eeg_sample[:, time])
@@ -488,15 +475,15 @@
         return source_out
             
     @staticmethod
     def robust_minmax_scaler(eeg):
         lower, upper = [np.percentile(eeg, 25), np.percentile(eeg, 75)]
         return (eeg-lower) / (upper-lower)
 
-    def predict(self, *args):
+    def predict(self, *args, verbose=1):
         ''' Predict sources from EEG data.
 
         Parameters
         ----------
         *args : 
             Can be either 
                 eeg : mne.Epochs/ numpy.ndarray
@@ -512,36 +499,36 @@
         outsource : either numpy.ndarray (if dtype='raw') or mne.SourceEstimate instance
         '''
         
         eeg, _ = self._handle_data_input(args)
 
         if isinstance(eeg, util.EVOKED_INSTANCES):
             # Ensure there are no extra channels in our EEG
-            eeg = eeg.pick_channels(self.fwd.ch_names)    
+            eeg = eeg.pick(self.fwd.ch_names)    
 
             sfreq = eeg.info['sfreq']
             tmin = eeg.tmin
             eeg = eeg.data
             # add empty trial dimension
             eeg = np.expand_dims(eeg, axis=0)
             if len(eeg.shape) == 2:
                 # add empty time dimension
                 eeg = np.expand_dims(eeg, axis=2)
         elif isinstance(eeg, util.EPOCH_INSTANCES):
             # Ensure there are no extra channels in our EEG
-            eeg = eeg.pick_channels(self.fwd.ch_names)
+            eeg = eeg.pick(self.fwd.ch_names)
             eeg.load_data()
 
             sfreq = eeg.info['sfreq']
             tmin = eeg.tmin
             eeg = eeg._data
         elif isinstance(eeg, list) and isinstance(eeg[0], util.EPOCH_INSTANCES):
             sfreq = eeg[0].info['sfreq']
             tmin = eeg[0].tmin
-            eeg = [e.get_data()[0] for e in eeg]
+            eeg = [e.get_data(copy=True)[0] for e in eeg]
             
         # else:
         #     msg = f'eeg must be of type <mne.EvokedArray> or <mne.epochs.EpochsArray>; got {type(eeg)} instead.'
         #     raise ValueError(msg)
         # Prepare EEG to ensure common average reference and appropriate scaling
         # eeg_prep =  self._prep_eeg(eeg)
         eeg_prep = self.scale_eeg(deepcopy(eeg))
@@ -560,72 +547,80 @@
                     time_slice_interp = interpolator.set_values(time_slice)()[::-1]
                     time_slice_interp = time_slice_interp[:, :, np.newaxis]
                     list_of_time_slices.append(time_slice_interp)
                 eeg_prep_interp[i] = np.stack(list_of_time_slices, axis=0)
                 eeg_prep_interp[i][np.isnan(eeg_prep_interp[i])] = 0
             eeg_prep = eeg_prep_interp
             del eeg_prep_interp
-            # print("shape of eeg_prep before prediciton: ", eeg_prep[0].shape)
+
             predicted_sources = self.predict_sources_interp(eeg_prep)
         else:
             # Predicted sources all in one go
-            # print("shape of eeg_prep before prediciton: ", eeg_prep[0].shape)
-            predicted_sources = self.predict_sources(eeg_prep)       
+            predicted_sources = self.predict_sources(eeg_prep)
 
         # Rescale Predicitons
         if self.rescale_sources.lower() == 'brent':
             predicted_sources_scaled = self._solve_p_wrap(predicted_sources, eeg)
         elif self.rescale_sources.lower() == 'rms':
             predicted_sources_scaled = self._scale_p_wrap(predicted_sources, eeg)
         else:
             print("Warning: <rescale_sources> is set to {self.rescale_sources}, but needs to be brent or rms. Setting to default (brent)")
             predicted_sources_scaled = self._solve_p_wrap(predicted_sources, eeg)
 
 
 
         # Convert sources (numpy.ndarrays) to mne.SourceEstimates objects
-        
+        if verbose>0:
+            eeg_hat = list()
+            for predicted_source in predicted_sources_scaled:
+                eeg_hat.append( self.leadfield @ predicted_source )
+            
+            residual_variances = [round(self.calc_residual_variance(M_hat, M), 2) for M_hat, M in zip(eeg_hat, eeg)]
+            print(f"Residual Variance(s): {residual_variances} [%]")
+
         predicted_source_estimate = [
             util.source_to_sourceEstimate(predicted_source_scaled, self.fwd, \
                 sfreq=sfreq, tmin=tmin, subject=self.subject) \
                 for predicted_source_scaled in predicted_sources_scaled]
         
         return predicted_source_estimate
 
+    def calc_residual_variance(self, M_hat, M):
+        return 100 *  np.sum( (M-M_hat)**2 ) / np.sum(M**2)
+
     def predict_sources(self, eeg):
         ''' Predict sources of 3D EEG (samples, channels, time) by reshaping 
         to speed up the process.
         
         Parameters
         ----------
         eeg : numpy.ndarray
             3D numpy array of EEG data (samples, channels, time)
         '''
         assert len(eeg[0].shape)==2, 'eeg must be a list of 2D numpy array of dim (channels, time)'
-
-        predicted_sources = [self.model.predict(e[np.newaxis, :, :])[0] for e in eeg]
-            
+        predicted_sources = [self.model.predict(e[:, np.newaxis], verbose=self.verbose)[:,0].T for e in eeg]
         # predicted_sources = np.swapaxes(predicted_sources,1,2)
-        predicted_sources = [np.swapaxes(src, 0, 1) for src in predicted_sources]
-        # print("shape of predicted sources: ", predicted_sources[0].shape)
+        # predicted_sources = [np.swapaxes(src, 0, 1) for src in predicted_sources]
+        # predicted_sources = [np.swapaxes(src, 0, 1) for src in predicted_sources]
+        
 
         return predicted_sources
 
     def predict_sources_interp(self, eeg):
         ''' Predict sources of 3D EEG (samples, channels, time) by reshaping 
         to speed up the process.
         
         Parameters
         ----------
         eeg : numpy.ndarray
             3D numpy array of EEG data (samples, channels, time)
         '''
         assert len(eeg[0].shape)==4, 'eeg must be a list of 4D numpy array of dim (time, height, width, 1)'
 
-        predicted_sources = [self.model.predict(e[np.newaxis, :, :])[0] for e in eeg]
+        predicted_sources = [self.model.predict(e[np.newaxis, :, :], verbose=self.verbose)[0] for e in eeg]
             
         # predicted_sources = np.swapaxes(predicted_sources,1,2)
         predicted_sources = [np.swapaxes(src, 0, 1) for src in predicted_sources]
         # print("shape of predicted sources: ", predicted_sources[0].shape)
 
         return predicted_sources
 
@@ -788,31 +783,31 @@
         function will either build:
 
         (1) A simple single hidden layer fully connected ANN for single time instance data
         (2) A LSTM network for spatio-temporal prediction
         '''
         if self.model_type.lower() == 'convdip':
             self._build_convdip_model()
+        elif self.model_type.lower() == "cnn":
+            self._build_cnn_model()
         elif self.model_type.lower() == 'fc':
             self._build_fc_model()
         elif self.model_type.lower() == 'lstm':
             self._build_temporal_model()
         else:
             self._build_temporal_model()
-        
 
         if self.verbose:
             self.model.summary()
     
     
     def _build_temporal_model(self):
         ''' Build the temporal artificial neural network model using LSTM layers.
         '''
         name = "LSTM Model"
-        print("werks3")
         self.model = keras.Sequential(name=name)
         tf.keras.backend.set_image_data_format('channels_last')
         input_shape = (None, self.n_channels)
         
         # LSTM layers
         if isinstance(self.n_lstm_units, (tuple, list)):
             self.n_lstm_units = self.n_lstm_units[0]
@@ -827,35 +822,35 @@
                     activation=self.activation_function), 
                     name='FC1')(inputs)
         fc1 = Dropout(self.dropout)(fc1)
         direct_out = TimeDistributed(Dense(self.n_dipoles, 
             activation="linear"),
             name='FC2')(fc1)
         # LSTM Path
-        lstm1 = Bidirectional(GRU(self.n_lstm_units, return_sequences=True, 
+        lstm1 = Bidirectional(LSTM(self.n_lstm_units, return_sequences=True, 
             input_shape=(None, self.n_dense_units), dropout=self.dropout), 
             name='LSTM1')(fc1)
         mask = TimeDistributed(Dense(self.n_dipoles, 
                     activation="sigmoid"), 
                     name='Mask')(lstm1)
         
         # Combination
         multi = multiply([direct_out, mask], name="multiply")
         self.model = tf.keras.Model(inputs=inputs, outputs=multi, name='Contextualizer')
+        if self.l1_reg is not None:
+            self.model.add_loss(self.l1_reg * self.l1_sparsity(multi))
         
     def _build_fc_model(self):
         ''' Build the temporal artificial neural network model using LSTM layers.
         '''
-        
-        name = "FC-model"
-        print("wrks4")
-        self.model = keras.Sequential(name=name)
+        # self.model = keras.Sequential(name=name)
         tf.keras.backend.set_image_data_format('channels_last')
         input_shape = (None, self.n_channels)
-        self.model.add(InputLayer(input_shape=input_shape, name='Input'))
+        # self.model.add(InputLayer(input_shape=input_shape, name='Input'))
+        inputs = tf.keras.Input(shape=input_shape, name='Input_FC')
         
   
         if not isinstance(self.dropout, (tuple, list)):
             dropout = [self.dropout]*self.n_lstm_layers
         else:
             dropout = self.dropout
         
@@ -865,151 +860,94 @@
             self.n_dense_units = [self.n_dense_units] * self.n_dense_layers
         
         if not isinstance(self.dropout, (tuple, list)):
             dropout = [self.dropout]*self.n_dense_layers
         else:
             dropout = self.dropout
         
-
+        add_to = inputs
         for i in range(self.n_dense_layers):
-            self.model.add(TimeDistributed(Dense(self.n_dense_units[i], 
-                activation=self.activation_function), name=f'FC_{i}'))
-            self.model.add(Dropout(dropout[i], name=f'Drop_{i}'))
+            dense = TimeDistributed(Dense(self.n_dense_units[i], 
+                activation=self.activation_function), name=f'FC_{i}')(add_to)
+            dense = Dropout(dropout[i], name=f'Drop_{i}')(dense)
+            add_to = dense
 
         # Final For-each layer:
-        self.model.add(TimeDistributed(
-            Dense(self.n_dipoles, activation='linear'), name='FC_Out')
-        )
+        out = TimeDistributed(Dense(self.n_dipoles, activation='linear'), name='FC_Out')(dense)
+        self.model = tf.keras.Model(inputs=inputs, outputs=out, name='FC_Model')
+        if self.l1_reg is not None:
+            self.model.add_loss(self.l1_reg * self.l1_sparsity(out))        
 
 
-        self.model.build(input_shape=input_shape)
+        # self.model.build(input_shape=input_shape)
 
-    # def _build_temporal_model(self):
-    #     ''' Build the temporal artificial neural network model using LSTM layers.
-    #     '''
-    #     if self.n_lstm_layers>0 and self.n_dense_layers>0:
-    #         name = "Mixed-model"
-    #     elif self.n_lstm_layers>0 and self.n_dense_layers==0:
-    #         name = "LSTM-model"
-    #     else:
-    #         name = "Dense-model"
-        
-    #     self.model = keras.Sequential(name='LSTM_v2')
-    #     tf.keras.backend.set_image_data_format('channels_last')
-    #     input_shape = (None, self.n_channels)
-    #     self.model.add(InputLayer(input_shape=input_shape, name='Input'))
-        
-    #     # LSTM layers
-    #     if not isinstance(self.n_lstm_units, (tuple, list)):
-    #         self.n_lstm_units = [self.n_lstm_units] * self.n_lstm_layers
-        
-    #     if not isinstance(self.dropout, (tuple, list)):
-    #         dropout = [self.dropout]*self.n_lstm_layers
-    #     else:
-    #         dropout = self.dropout
-        
-    #     for i in range(self.n_lstm_layers):
-    #         self.model.add(Bidirectional(LSTM(self.n_lstm_units[i], 
-    #             return_sequences=True, input_shape=input_shape),
-    #             name=f'RNN_{i}'))
-    #         self.model.add(Dropout(dropout[i], name=f'Dropout_{i}'))
-
-    #     # Hidden Dense layer(s):
-    #     if not isinstance(self.n_dense_units, (tuple, list)):
-    #         self.n_dense_units = [self.n_dense_units] * self.n_dense_layers
-        
-    #     if not isinstance(self.dropout, (tuple, list)):
-    #         dropout = [self.dropout]*self.n_dense_layers
-    #     else:
-    #         dropout = self.dropout
-        
-
-    #     for i in range(self.n_dense_layers):
-    #         self.model.add(TimeDistributed(Dense(self.n_dense_units[i], 
-    #             activation=self.activation_function), name=f'FC_{i}'))
-    #         self.model.add(Dropout(dropout[i], name=f'Drop_{i}'))
-
-    #     # Final For-each layer:
-    #     self.model.add(TimeDistributed(
-    #         Dense(self.n_dipoles, activation='linear'), name='FC_Out')
-    #     )
-
-    #     self.model.build(input_shape=input_shape)
-
-
-    # def _build_temporal_model_v3(self):
-    #     ''' A mixed dense / LSTM network, inspired by:
-    #     "Deep Burst Denoising" (Godarg et al., 2018)
-    #     '''
-    #     inputs = keras.Input(shape=(None, self.n_channels), name='Input')
-    #     # SINGLE TIME FRAME PATH
-    #     fc1 = TimeDistributed(Dense(self.n_dense_units, 
-    #         activation=self.activation_function), 
-    #         name='FC1')(inputs)
-    #     fc1 = Dropout(self.dropout, name='Dropout1')(fc1)
-
-    #     # fc2 = TimeDistributed(Dense(self.n_dipoles,
-    #     #     activation=self.activation_function), 
-    #     #     name='FC2')(fc1)
-    #     # fc2 = Dropout(self.dropout, name='Dropout2')(fc2)
-
-    #     # model_s = keras.Model(inputs=inputs, outputs=fc2, 
-    #     #     name='single_time_ frame_model')
-
-    #     # MULTI TIME FRAME PATH
-    #     lstm1 = Bidirectional(LSTM(self.n_lstm_units, return_sequences=True, 
-    #         input_shape=(None, self.n_dense_units), dropout=self.dropout, 
-    #         activation=self.activation_function), name='LSTM1')(inputs)
-
-    #     concat = concatenate([lstm1, fc1], name='Concat')
-
-    #     lstm2 = Bidirectional(LSTM(self.n_lstm_units, return_sequences=True, 
-    #         input_shape=(None, self.n_dense_units), dropout=self.dropout, 
-    #         activation=self.activation_function), name='LSTM2')(concat)
+    def _build_cnn_model(self):
+        tf.keras.backend.image_data_format() == 'channels_last'
+        input_shape = (None, self.n_channels, 1)
 
-    #     output = TimeDistributed(Dense(self.n_dipoles), name='FC_Out')(lstm2)
-    #     model_m = keras.Model(inputs=inputs, outputs=output, name='LSTM_v3')
+        inputs = tf.keras.Input(shape=input_shape, name='Input_CNN')
+        fc = TimeDistributed(Conv1D(self.n_filters, self.n_channels, activation=self.activation_function, name="HL_D1"))(inputs)
+        fc = TimeDistributed(Flatten())(fc)
+            
+        # LSTM path
+        lstm1 = Bidirectional(GRU(self.n_lstm_units, return_sequences=True), name='GRU')(fc)
+        mask = TimeDistributed(Dense(self.n_dipoles, activation="sigmoid"), name='Mask')(lstm1)
+
+        direct_out = TimeDistributed(Dense(self.n_dipoles, activation="tanh", name="Output_Final"))(fc)
+        multi = multiply([direct_out, mask], name="multiply")
+
+        self.model = tf.keras.Model(inputs=inputs, outputs=multi, name='Contextual_CNN_Model')
+        if self.l1_reg is not None:
+            self.model.add_loss(self.l1_reg * self.l1_sparsity(multi))
+        # model.compile(loss=tf.keras.losses.CosineSimilarity(), optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))
 
-    #     self.model = model_m
-    
-        
     def _build_convdip_model(self):
-        self.model = keras.Sequential(name='ConvDip-model')
+        # self.model = keras.Sequential(name='ConvDip-model')
         tf.keras.backend.set_image_data_format('channels_last')
-        # Some definitions
         input_shape = (None, *self.interp_channel_shape, 1)
-        
+        inputs = tf.keras.Input(shape=input_shape, name='Input_ConvDip')
+        # Some definitions
+              
 
         # Hidden Dense layer(s):
         if not isinstance(self.n_dense_units, (tuple, list)):
             self.n_dense_units = [self.n_dense_units] * self.n_dense_layers
         
         if not isinstance(self.dropout, (tuple, list)):
             dropout = [self.dropout]*(self.n_dense_layers+self.n_lstm_layers)
         else:
             dropout = self.dropout
 
-        self.model.add(InputLayer(input_shape=input_shape, name='Input'))
+        # self.model.add(InputLayer(input_shape=input_shape, name='Input'))
+        add_to = inputs
         for i in range(self.n_lstm_layers):
-            self.model.add(TimeDistributed(Conv2D(self.n_filters, self.kernel_size, activation=self.activation_function, name=f"Conv2D_{i}")))
-            self.model.add(Dropout(dropout[i], name=f'Drop_conv2d_{i}'))
+            conv = TimeDistributed(Conv2D(self.n_filters, self.kernel_size, activation=self.activation_function, name=f"Conv2D_{i}"))(add_to)
+            conv = Dropout(dropout[i], name=f'Drop_conv2d_{i}')(conv)
+            add_to = conv
 
 
-        self.model.add(TimeDistributed(Flatten()))
 
+        flat = TimeDistributed(Flatten())(conv)
+        add_to = flat
         for i in range(self.n_dense_layers):
-            self.model.add(TimeDistributed(Dense(self.n_dense_units[i], activation=self.activation_function, name=f'FC_{i}')))
-            self.model.add(Dropout(dropout[i], name=f'Drop_FC_{i}'))
+            dense = TimeDistributed(Dense(self.n_dense_units[i], activation=self.activation_function, name=f'FC_{i}'))(add_to)
+            dense = Dropout(dropout[i], name=f'Drop_FC_{i}')(dense)
+            add_to = dense
 
         # Outout Layer
-        self.model.add(TimeDistributed(Dense(self.n_dipoles, activation='linear'), name='FC_Out'))
-
-        self.model.build(input_shape=input_shape)
-
+        out = TimeDistributed(Dense(self.n_dipoles, activation='linear'), name='FC_Out')(dense)
+        self.model = tf.keras.Model(inputs=inputs, outputs=out, name='ConvDip_Model')
+        if self.l1_reg is not None:
+            self.model.add_loss(self.l1_reg * self.l1_sparsity(out))
         
+
+    @staticmethod
+    def l1_sparsity(x):
+        new_x = tf.math.l2_normalize(x)
+        return K.mean(K.abs(new_x))
         
   
 
     def _freeze_lstm(self):
         for i, layer in enumerate(self.model.layers):
             if 'LSTM' in layer.name or 'RNN' in layer.name:
                 print(f'freezing {layer.name}')
@@ -1127,26 +1065,29 @@
             return y_est
         y_est = np.squeeze(np.array(y_est))
         x_true = np.squeeze(np.array(x_true))
         # Get EEG from predicted source using leadfield
         x_est = np.matmul(self.leadfield, y_est)
 
         # optimize forward solution
-        tol = 1e-3
+        tol = 1e-9
         options = dict(maxiter=1000, disp=False)
 
         # base scaling
         rms_est = np.mean(np.abs(x_est))
         rms_true = np.mean(np.abs(x_true))
         base_scaler = rms_true / rms_est
 
         
         opt = minimize_scalar(self.correlation_criterion, args=(self.leadfield, y_est* base_scaler, x_true), \
             bounds=(0, 1), method='bounded', options=options, tol=tol)
         
+        # opt = minimize_scalar(self.correlation_criterion, args=(self.leadfield, y_est* base_scaler, x_true), \
+        #     bounds=(0, 1), method='L-BFGS-B', options=options, tol=tol)
+
         scaler = opt.x
         y_scaled = y_est * scaler * base_scaler
         return y_scaled
 
     @staticmethod
     def correlation_criterion(scaler, leadfield, y_est, x_true):
         ''' Perform forward projections of a source using the leadfield.
@@ -1209,28 +1150,268 @@
         except:
             print("Load model did not work using custom_objects. Now trying it without...")
             self.model = tf.keras.models.load_model(new_path)
         
         return self
 
     @staticmethod
-    def make_interpolator(elec_pos, res=9, ch_type='eeg'):
+    def make_interpolator(elec_pos, res=9, ch_type='eeg', image_interp="linear"):
         extrapolate = _check_extrapolate('auto', ch_type)
         sphere = sphere = _check_sphere(None)
         outlines = 'head'
         outlines = _make_head_outlines(sphere, elec_pos, outlines, (0., 0.))
         border = 'mean'
         extent, Xi, Yi, interpolator = _setup_interp(
-            elec_pos, res, extrapolate, sphere, outlines, border)
+            elec_pos, res, image_interp, extrapolate, outlines, border)
         interpolator.set_locations(Xi, Yi)
 
         return interpolator
 
     
 
+class CovNet:
+    ''' Class for the Covariance-based Convolutional Neural Network (CovCNN) for EEG inverse solutions.
+    
+    Attributes
+    ----------
+    forward : mne.Forward
+        The mne-python Forward model instance.
+    '''
+
+    def __init__(self, forward, name="Cov-CNN", n_filters="auto", 
+                activation_function="tanh", batch_size="auto", 
+                n_timepoints=20, batch_repetitions=10,
+                learning_rate=1e-3, loss="cosine_similarity",
+                n_sources=10, n_orders=2, epsilon=0.5, 
+                snr_range=(1,100), alpha="auto", verbose=0, **kwargs):
+        ''' Calculate inverse operator.
+
+        Parameters
+        ----------
+        forward : mne.Forward
+            The mne-python Forward model instance.
+        alpha : float
+            The regularization parameter.
+        
+        Return
+        ------
+        self : object returns itself for convenience
+        '''
+        # Leadfield
+        self.forward = forward
+        self.leadfield = deepcopy(forward["sol"]["data"])
+        self.leadfield -= self.leadfield.mean(axis=0)
+
+        n_channels, n_dipoles = self.leadfield.shape
+        if batch_size == "auto":
+            batch_size = n_dipoles
+        if n_filters == "auto":
+            n_filters = n_channels
+            
+        # Store Parameters
+        
+        
+        # Architecture
+        self.name = name
+        self.n_filters = n_filters
+        self.activation_function = activation_function
+        # Training
+        self.batch_size = batch_size
+        self.learning_rate = learning_rate
+        self.loss = loss
+        # Training Data
+        self.n_timepoints = n_timepoints
+        self.n_sources = n_sources
+        self.n_orders = n_orders
+        self.batch_repetitions = batch_repetitions
+        self.snr_range = snr_range
+        # Inference
+        self.epsilon = epsilon
+        # Other
+        self.verbose = verbose
+        print("Build Model:..")
+        self.build_model()
+        
+
+    def predict(self, evoked) -> mne.SourceEstimate:
+        source_mat = self.apply_model(evoked)
+        stc = self.source_to_object(source_mat, evoked)
+
+        return stc
+
+    def apply_model(self, evoked) -> np.ndarray:
+        y = deepcopy(evoked.data)
+        y -= y.mean(axis=0)
+        print("werks")
+        # y /= np.linalg.norm(y, axis=0)
+
+        n_channels, n_times = y.shape
+
+        # Compute Data Covariance Matrix
+        C = y@y.T
+        # Scale
+        C /= abs(C).max()
+        
+
+        # Add empty batch and (color-) channel dimension
+        C = C[np.newaxis, :, :, np.newaxis]
+        gammas = self.model.predict(C, verbose=self.verbose)[0]
+        gammas /= gammas.max()
+
+        
+        
+
+
+        # Select dipole indices
+        gammas[gammas<self.epsilon] = 0
+        dipole_idc = np.where(gammas!=0)[0]
+        print("Active dipoles: ", len(dipole_idc))
+
+        # 1) Calculate weighted minimum norm solution at active dipoles
+        n_dipoles = len(gammas)
+        y = deepcopy(evoked.data)
+        y -= y.mean(axis=0)
+        x_hat = np.zeros((n_dipoles, n_times))
+        L = self.leadfield[:, dipole_idc]
+        W = np.diag(np.linalg.norm(L, axis=0))
+        x_hat[dipole_idc, :] = np.linalg.inv(L.T @ L + W.T@W) @ L.T @ y
+
+        
+        return x_hat        
+        
+        
+    def fit(self, sim, patience=7, validation_split=0.05, epochs=300, return_history=True):
+        callbacks = [tf.keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True),]
+        
+        x_train = np.stack(
+            self.prep_x([ep.average().data for ep in sim.eeg_data])
+            , axis=0)
+        y_train = np.stack(
+            self.prep_y([stc.data for stc in sim.source_data])
+            , axis=0)
+        # print(x_train.shape, y_train.shape)
+        
+        history = self.model.fit(x_train, y_train, epochs=epochs, 
+            validation_split=validation_split, callbacks=callbacks)
+
+        if return_history:
+            return self, history
+        return self
+    def prep_y(self, y):
+        n_samples = len(y)
+        y_scaled = []
+
+        for i in range(n_samples):
+            y_sample = y[i]
+
+            y_sample = np.mean(abs(y_sample), axis=1)
+            thr = y_sample.max()*1e-3
+            y_sample = (y_sample>thr).astype(float)
+            y_scaled.append(y_sample)
+        return y_scaled
+
+
+    def prep_x(self, x):
+        n_samples = len(x)
+        C_scaled = []
+        for i in range(n_samples):
+            x_sample = x[i]
+            # Common Average Reference
+            x_sample -= x_sample.mean(axis=0)
+            x_sample /= np.linalg.norm(x_sample, axis=0)
+            C = x_sample @ x_sample.T
+            C /= abs(C).max()
+            C_scaled.append(C)
+        return C_scaled
+            
+
+    def build_model(self,):
+        n_channels, n_dipoles = self.leadfield.shape
+
+        inputs = tf.keras.Input(shape=(n_channels, n_channels, 1), name='Input')
+
+        cnn1 = Conv2D(self.n_filters, (1, n_channels),
+                    activation=self.activation_function, padding="valid",
+                    name='CNN1')(inputs)
+
+        flat = Flatten()(cnn1)
+        
+        fc1 = Dense(200, 
+            activation=self.activation_function, 
+            name='FC1')(flat)
+        out = Dense(n_dipoles, 
+            activation="sigmoid", 
+            name='Output')(fc1)
+
+        model = tf.keras.Model(inputs=inputs, outputs=out, name='CovCNN')
+        model.compile(loss=self.loss, optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))
+        if self.verbose > 0:
+            model.summary()
+        
+        self.model = model
+ 
+    def source_to_object(self, source_mat, evoked):
+        ''' Converts the source_mat matrix to an mne.SourceEstimate object '''
+        # Convert source to mne.SourceEstimate object
+        source_model = self.forward['src']
+        vertices = [source_model[0]['vertno'], source_model[1]['vertno']]
+        tmin = evoked.tmin
+        sfreq = evoked.info["sfreq"]
+        tstep = 1/sfreq
+        subject = evoked.info["subject_info"]
+
+        if type(subject) == dict:
+            subject = "bst_raw"
+
+        if subject is None:
+            subject = "fsaverage"
+        
+        stc = mne.SourceEstimate(source_mat, vertices, tmin=tmin, tstep=tstep, subject=subject, verbose=self.verbose)
+        return stc
+
+    def save(self, path, name='model'):
+        # get list of folders in path
+        list_of_folders = os.listdir(path)
+        model_ints = []
+
+        for folder in list_of_folders:
+            full_path = os.path.join(path, folder)
+            if not os.path.isdir(full_path):
+                continue
+            if folder.startswith(name):
+                new_integer = int(folder.split('_')[-1])
+                model_ints.append(new_integer)
+        if len(model_ints) == 0:
+            model_name = f'\\{name}_0'
+        else:
+            model_name = f'\\{name}_{max(model_ints)+1}'
+
+        new_path = path+model_name
+        os.mkdir(new_path)
+
+        # Save model only
+        self.model.save(new_path)
+
+        
+        # Save rest
+        # Delete model since it is not serializable
+        self.model = None
+
+        with open(new_path + '\\instance.pkl', 'wb') as f:
+            pkl.dump(self, f)
+        
+        # Attach model again now that everything is saved
+        try:
+            self.model = tf.keras.models.load_model(new_path, custom_objects={'loss': self.loss})
+        except:
+            print("Load model did not work using custom_objects. Now trying it without...")
+            self.model = tf.keras.models.load_model(new_path)
+        
+        return self
+
 def build_nas_lstm(hp):
     ''' Find optimal model using keras tuner.
     '''
     n_dipoles = 1284
     n_channels = 61
     n_lstm_layers = hp.Int("lstm_layers", min_value=0, max_value=3, step=1)
     n_dense_layers = hp.Int("dense_layers", min_value=0, max_value=3, step=1)
@@ -1274,267 +1455,8 @@
         loss="huber",
         # metrics=[tf.keras.metrics.AUC()],
         # metrics=[evaluate.modified_auc_metric()],
         metrics=[evaluate.auc],
     )
     return model
 
-# class EnsembleNet:
-#     ''' Uses ensemble of neural networks to perform predictions
-#     Attributes
-#     ----------
-#     nets : list
-#         a list of instances of the Net class
-#     ensemble_mode : str
-#         Decides how the various predictions will be combined.
-#         'average' : average all predictions with equal weight
-    
-#     Methods
-#     -------
-#     predict : performs predictions with each Net instance and combines them.
-#     vote_average : the implementation of the ensemble_mode 'average'
-
-#     Examples
-#     --------
-#     ### Build two Nets nad train them
-#     k = 2  # number of models
-#     nets = [Net(fwd).fit(simulation.eeg_data, simulation.source_data) for _ in range(k)]
-#     ### Combine them into an EnsembleNet
-#     ens_net = EnsembleNet(nets)
-#     ### Perform prediction
-#     y_hat = nets[0].predict(simulation_test)
-#     y_hat_ens = ens_net.predict(simulation_test.eeg_data)
-#     ### Plot result
-#     a = simulation_test.source_data.plot(**plot_params)  # Ground truth
-#     b = y_hat.plot(**plot_params)  # single-model prediction
-#     c = y_hat_ens.plot(**plot_params)  # ensemble predicion
-
-
-
-#     '''
-#     def __init__(self, nets, ensemble_mode='average'):
-#         self.nets = nets
-#         self.ensemble_mode = ensemble_mode
-        
-#         if ensemble_mode == 'average':
-#             self.vote = self.vote_average
-#         # if ensemble_mode == 'stack':
-#         #     self.vote = self.vote_stack
-#         else:
-#             msg = f'ensemble_mode {ensemble_mode} not supported'
-#             raise AttributeError(msg)
-        
-
-#     def predict(self, *args):
-#         predictions = [net.predict(args[1]) for net in self.nets]
-#         predictions_data = np.stack([prediction.data for prediction in predictions], axis=0)
-        
-#         ensemble_prediction = predictions[0]
-#         ensemble_prediction.data = self.vote(predictions_data)
-
-#         return ensemble_prediction
-
-#     def vote_average(self, predictions_data):
-#         return np.mean(predictions_data, axis=0)
-
-# class BoostNet:
-#     ''' The Boosted neural network class that creates and trains the boosted model. 
-        
-#     Attributes
-#     ----------
-#     fwd : mne.Forward
-#         the mne.Forward forward model class.
-#     n_nets : int
-#         The number of neural networks to use.
-#     n_layers : int
-#         Number of hidden layers in the neural network.
-#     n_neurons : int
-#         Number of neurons per hidden layer.
-#     activation_function : str
-#         The activation function used for each fully connected layer.
-
-#     Methods
-#     -------
-#     fit : trains the neural network with the EEG and source data
-#     train : trains the neural network with the EEG and source data
-#     predict : perform prediciton on EEG data
-#     evaluate : evaluate the performance of the model
-#     '''
-
-#     def __init__(self, fwd, n_nets=5, n_layers=1, n_neurons=128, 
-#         activation_function='swish', verbose=False):
-
-#         self.nets = [Net(fwd, n_layers=n_layers, n_neurons=n_neurons, 
-#             activation_function=activation_function, verbose=verbose) 
-#             for _ in range(n_nets)]
-
-#         self.linear_regressor = linear_model.LinearRegression()
-
-#         self.verbose=verbose
-#         self.n_nets = n_nets
-
-#     def fit(self, *args, **kwargs):
-#         ''' Train the boost model.
-
-#         Parameters
-#         ----------
-#         *args : esinet.simulation.Simulation
-#             Can be either 
-#                 eeg : mne.Epochs/ numpy.ndarray
-#                     The simulated EEG data
-#                 sources : mne.SourceEstimates/ list of mne.SourceEstimates
-#                     The simulated EEG data
-#                 or
-#                 simulation : esinet.simulation.Simulation
-#                     The Simulation object
-
-#         **kwargs
-#             Arbitrary keyword arguments.
-
-#         Return
-#         ------
-#         self : BoostNet()
-#         '''
-
-#         eeg, sources = self._handle_data_input(args)
-#         self.subject = sources.subject if type(sources) == mne.SourceEstimate else sources[0].subject
-
-#         if self.verbose:
-#             print("Fit neural networks")
-#         self._fit_nets(eeg, sources, **kwargs)
-
-#         ensemble_predictions, _ = self._get_ensemble_predictions(eeg, sources)
-           
-#         if self.verbose:
-#             print("Fit regressor")
-#         # Train linear regressor to combine predictions
-#         self.linear_regressor.fit(ensemble_predictions, sources.data.T)
-
-#         return self
-    
-#     def predict(self, *args):
-#         ''' Perform prediction of sources based on EEG data using the Boosted Model.
-        
-#         Parameters
-#         ----------
-#         *args : 
-#             Can be either 
-#                 eeg : mne.Epochs/ numpy.ndarray
-#                     The simulated EEG data
-#                 sources : mne.SourceEstimates/ list of mne.SourceEstimates
-#                     The simulated EEG data
-#                 or
-#                 simulation : esinet.simulation.Simulation
-#                     The Simulation object
-#         **kwargs
-#             Arbitrary keyword arguments.
-        
-#         Return
-#         ------
-#         '''
-
-#         eeg, sources = self._handle_data_input(args)
-
-#         ensemble_predictions, y_hats = self._get_ensemble_predictions(eeg, sources)
-#         prediction = np.clip(self.linear_regressor.predict(ensemble_predictions), a_min=0, a_max=np.inf)
-        
-#         y_hat = y_hats[0]
-#         y_hat.data = prediction.T
-#         return y_hat
-
-#     def evaluate_mse(self, *args):
-#         ''' Evaluate the model regarding mean squared error
-        
-#         Parameters
-#         ----------
-#         *args : 
-#             Can be either 
-#                 eeg : mne.Epochs/ numpy.ndarray
-#                     The simulated EEG data
-#                 sources : mne.SourceEstimates/ list of mne.SourceEstimates
-#                     The simulated EEG data
-#                 or
-#                 simulation : esinet.simulation.Simulation
-#                     The Simulation object
-
-#         Return
-#         ------
-#         mean_squared_errors : numpy.ndarray
-#             The mean squared error of each sample
-
-#         Example
-#         -------
-#         net = BoostNet()
-#         net.fit(simulation)
-#         mean_squared_errors = net.evaluate(simulation)
-#         print(mean_squared_errors.mean())
-#         '''
-
-#         eeg, sources = self._handle_data_input(args)
-#         y_hat = self.predict(eeg, sources).data
-#         y_true = sources.data
-#         mean_squared_errors = np.mean((y_hat - y_true)**2, axis=0)
-#         return mean_squared_errors
-
-
-#     def _get_ensemble_predictions(self, *args):
-
-#         eeg, sources = self._handle_data_input(args)
-
-#         y_hats = [subnet.predict(eeg, sources) for subnet in self.nets]
-#         ensemble_predictions = np.stack([y_hat[0].data for y_hat in y_hats], axis=0).T
-#         ensemble_predictions = ensemble_predictions.reshape(ensemble_predictions.shape[0], np.prod((ensemble_predictions.shape[1], ensemble_predictions.shape[2])))
-#         return ensemble_predictions, y_hats
-
-#     def _fit_nets(self, *args, **kwargs):
-
-#         eeg, sources = self._handle_data_input(args)
-#         n_samples = eeg.get_data().shape[0]
-#         # sample_weight = np.ones((sources._data.shape[1]))
-        
-#         for net in self.nets:
-#             sample_idc = np.random.choice(np.arange(n_samples), 
-#                 int(0.8*n_samples), replace=True)
-#             eeg_bootstrap = eeg.copy()[sample_idc]
-#             sources_bootstrap = sources.copy()
-#             sources_bootstrap.data = sources_bootstrap.data[:, sample_idc]
-#             net.fit(eeg_bootstrap, sources_bootstrap, **kwargs)#, sample_weight=sample_weight)
-#             # sample_weight = net.evaluate_mse(eeg, sources)
-#             # print(f'new sample weights: mean={sample_weight.mean()} +- {sample_weight.std()}')
-
-        
-#     def _handle_data_input(self, arguments):
-#         ''' Handles data input to the functions fit() and predict().
-        
-#         Parameters
-#         ----------
-#         arguments : tuple
-#             The input arguments to fit and predict which contain data.
-        
-#         Return
-#         ------
-#         eeg : mne.Epochs
-#             The M/EEG data.
-#         sources : mne.SourceEstimates/list
-#             The source data.
-
-#         '''
-#         if len(arguments) == 1:
-#             if isinstance(arguments[0], (mne.Epochs, mne.Evoked, mne.io.Raw, mne.EpochsArray, mne.EvokedArray, mne.epochs.EpochsFIF)):
-#                 eeg = arguments[0]
-#                 sources = None
-#             else:
-#                 simulation = arguments[0]
-#                 eeg = simulation.eeg_data
-#                 sources = simulation.source_data
-#                 # msg = f'First input should be of type simulation or Epochs, but {arguments[1]} is {type(arguments[1])}'
-#                 # raise AttributeError(msg)
-
-#         elif len(arguments) == 2:
-#             eeg = arguments[0]
-#             sources = arguments[1]
-#         else:
-#             msg = f'Input is {type()} must be either the EEG data and Source data or the Simulation object.'
-#             raise AttributeError(msg)
-
-#         return eeg, sources
-
+
```

## esinet/simulation.py

```diff
@@ -4,31 +4,32 @@
 from scipy.spatial.distance import cdist
 # import pickle as pkl
 import dill as pkl
 import random
 from joblib import Parallel, delayed
 # from tqdm.notebook import tqdm
 from tqdm import tqdm
-
+from mne.channels.layout import _find_topomap_coords
 import colorednoise as cn
 import mne
 from time import time
 from . import util
 
 DEFAULT_SETTINGS = {
     'method': 'standard',
     'number_of_sources': (1, 25),
     'extents':  (1, 50),  # in millimeters
     'amplitudes': (1e-3, 100),
     'shapes': 'mixed',
     'duration_of_trial': 1.0,
     'sample_frequency': 100,
     'target_snr': (1, 20),
-    'beta': (0.5, 3),  # (0, 3),
-    'exponent': 3,
+    'beta': (0, 5), 
+    'beta_noise': (0, 5),
+    'beta_source': (0, 5),
     'source_spread': "mixed",
     'source_number_weighting': True,
     'source_time_course': "random",
 }
 
 class Simulation:
     ''' Simulate and hold source and M/EEG data.
@@ -89,30 +90,42 @@
         # self.info['sfreq'] = self.settings['sample_frequency']
         self.prepare_simulation_info()
         self.subject = self.fwd['src'][0]['subject_his_id']
         self.n_jobs = n_jobs
         self.parallel = parallel
         self.verbose = verbose
         self.diams = None
+        
     
     def __add__(self, other):
         new_object = deepcopy(self)
         new_object.source_data.extend(other.source_data)
         new_object.eeg_data.extend(other.eeg_data)
-        new_object.simulation_info.append(other.simulation_info)
+        new_object.simulation_info = pd.concat([
+            new_object.simulation_info, 
+            other.simulation_info
+            ])
+        #  Deprecated
+        # new_object.simulation_info.append(other.simulation_info)
         new_object.n_samples += other.n_samples
         if new_object.settings["method"] != other.settings["method"]:
             new_object.settings["method"] = "mixed"
         return new_object
         
     def check_info(self, info):
-        self.info = info.pick_channels(self.fwd.ch_names, ordered=True)
+        n_chans = len(info['ch_names'])
+        data = np.zeros((n_chans, 1))  
+        evoked = mne.EvokedArray(data, info, verbose=0)
+
+        new_info = evoked.pick(self.fwd.ch_names).info
+        self.info = new_info
+        
 
     def prepare_simulation_info(self):
-        self.simulation_info = pd.DataFrame(columns=['number_of_sources', 'positions', 'extents', 'amplitudes', 'shapes', 'target_snr', 'betas', 'duration_of_trials'])
+        self.simulation_info = pd.DataFrame(columns=['number_of_sources', 'positions', 'extents', 'amplitudes', 'shapes', 'target_snr', 'betas', 'betas_noise', 'duration_of_trials', 'beta_source'])
 
     def simulate(self, n_samples=10000):
         ''' Simulate sources and EEG data'''
         self.n_samples = n_samples
         self.source_data = self.simulate_sources(n_samples)
         self.eeg_data = self.simulate_eeg()
 
@@ -188,23 +201,23 @@
         shape = (n,n,n,n_time)
         
         x = np.linspace(self.pos[:, 0].min(), self.pos[:, 0].max(), num=shape[0])
         y = np.linspace(self.pos[:, 1].min(), self.pos[:, 1].max(), num=shape[1])
         z = np.linspace(self.pos[:, 2].min(), self.pos[:, 2].max(), num=shape[2])
         k_neighbors = 5
         grid = np.stack(np.meshgrid(x,y,z, indexing='ij'), axis=0)
-        grid_flat = grid.reshape(grid.shape[0], np.product(grid.shape[1:])).T
+        grid_flat = grid.reshape(grid.shape[0], np.prod(grid.shape[1:])).T
         neighbor_indices = np.stack([
             np.argsort(np.sqrt(np.sum((grid_flat - coords)**2, axis=1)))[:k_neighbors] for coords in self.pos
         ], axis=0)
 
         self.grid = {
             "shape": shape,
             "k_neighbors": k_neighbors,
-            "exponent": self.settings["exponent"],
+            "exponent": self.settings["beta_source"],
             "x": x,
             "y": y,
             "z": z,
             "grid": grid,
             "grid_flat": grid_flat,
             "neighbor_indices": neighbor_indices
         }
@@ -218,17 +231,24 @@
             self.settings['duration_of_trial'], dtype=float)
         n_time = np.clip(int(round(duration_of_trial * self.info['sfreq'])), 1, None)
         if len(src_3d.shape) == 3:
             src_3d = src_3d[:,:,:,np.newaxis]
         src = np.zeros((self.pos.shape[0], n_time))
         for i in range(n_time):
             src[:, i] = util.vol_to_src(self.grid["neighbor_indices"], src_3d[:, :, :, i], self.pos)
-        
-        d = dict(number_of_sources=np.nan, positions=[np.nan], extents=[np.nan], amplitudes=[np.nan], shapes=[np.nan], target_snr=0, duration_of_trials=duration_of_trial)
-        self.simulation_info = self.simulation_info.append(d, ignore_index=True)
+
+        d = dict(number_of_sources=np.nan, positions=[np.nan], extents=[np.nan], amplitudes=[np.nan], shapes=[np.nan], target_snr=0, duration_of_trials=duration_of_trial, beta_source=exponent)
+        df_new = pd.DataFrame(columns=self.simulation_info.columns)
+        for key, val in d.items():
+            df_new.loc[0, key] = val
+        df_new.reset_index(drop=True)
+
+        self.simulation_info = pd.concat([self.simulation_info, df_new])
+        # deprecated soon:
+        # self.simulation_info = self.simulation_info.append(d, ignore_index=True)
         return src
         
     def sources_to_sourceEstimates(self, source_data):
         template = util.source_to_sourceEstimate(source_data[0], 
                     self.fwd, sfreq=self.settings['sample_frequency'], 
                     subject=self.subject)
         sources = []
@@ -281,15 +301,15 @@
         if not self.settings["source_number_weighting"] or isinstance(self.settings["number_of_sources"], (float, int)):
             number_of_sources = self.get_from_range(
                 self.settings['number_of_sources'], dtype=int)
         else:
             population = np.arange(*self.settings["number_of_sources"])
             weights = 1 / population
             weights /= weights.sum()
-            number_of_sources = random.choices(population=population,weights=weights,k=1)[0]
+            number_of_sources = random.choices(population=population,weights=weights, k=1)[0]
 
         
         if self.settings["source_spread"] == 'mixed':
             source_spreads = [np.random.choice(['region_growing', 'spherical']) for _ in range(number_of_sources)]
         else:
             source_spreads = [self.settings["source_spread"] for _ in range(number_of_sources)]
 
@@ -313,25 +333,29 @@
         
         src_centers = np.random.choice(np.arange(self.pos.shape[0]), \
             number_of_sources, replace=False)
         duration_of_trial = self.get_from_range(
             self.settings['duration_of_trial'], dtype=float
         )
         signal_length = int(round(self.settings['sample_frequency']*duration_of_trial))
-
+        betas = []
         if signal_length > 1:
             signals = []
             
             if self.settings["source_time_course"].lower() == "pulse":
                 signals = [self.get_biphasic_pulse(signal_length) for _ in range(number_of_sources)]
             else:
+                
                 for _ in range(number_of_sources):
-                    signal = cn.powerlaw_psd_gaussian(self.get_from_range(self.settings['beta'], dtype=float), signal_length) 
+                    beta = self.get_from_range(self.settings['beta'], dtype=float)
+                    signal = cn.powerlaw_psd_gaussian(beta, signal_length) 
                     signal /= np.max(np.abs(signal))
                     signals.append(signal)
+                    betas.append(beta)
+                    
             
             sample_frequency = self.settings['sample_frequency']
         else:  # else its a single instance
             sample_frequency = 0
             signal_length = 1
             signals = list(np.random.choice([-1, 1], number_of_sources))
         
@@ -384,18 +408,24 @@
                         activity = np.expand_dims(activity, axis=1)
                 source[d, :] += activity 
             else:
                 msg = BaseException("shape must be of type >string< and be either >gaussian< or >flat<.")
                 raise(msg)
         
         # Document the sample
-        d = dict(number_of_sources=number_of_sources, positions=self.pos[src_centers], extents=extents, amplitudes=amplitudes, shapes=shapes, target_snr=0, duration_of_trials=duration_of_trial)
-        self.simulation_info = self.simulation_info.append(d, ignore_index=True)
-        # self.simulation_info = pd.concat([self.simulation_info, d])
+        d = dict(number_of_sources=number_of_sources, positions=self.pos[src_centers], extents=extents, amplitudes=amplitudes, shapes=shapes, target_snr=0, duration_of_trials=duration_of_trial, betas=betas)
+        df_new = pd.DataFrame(columns=self.simulation_info.columns)
+        for key, val in d.items():
+            df_new.loc[0, key] = val
+        df_new.reset_index(drop=True)
+
+        self.simulation_info = pd.concat([self.simulation_info, df_new])
         
+        # deprecated soon:
+        # self.simulation_info = self.simulation_info.append(d, ignore_index=True)
         return source
 
     def simulate_eeg(self):
         ''' Create EEG of specified number of trials based on sources and some SNR.
         Parameters
         -----------
         sourceEstimates : list 
@@ -418,103 +448,94 @@
         Return
         -------
         epochs : list
                 list of either mne.Epochs objects or list of raw EEG data 
                 (see argument <return_raw_data> to change output)
         '''
 
-        n_simulation_trials = 20
          
         # Desired Dim of sources: (samples x dipoles x time points)
         # unpack numpy array of source data
         # print(type(self.source_data))
         # if isinstance(self.source_data, (list, tuple)):
         #     sources = np.stack([source.data for source in self.source_data], axis=0)
         # else:
         #     sources = self.source_data.data.T
 
         # if there is no temporal dimension...
         for i, source in enumerate(self.source_data):
             if len(source.shape) == 1:
                 self.source_data[i] = np.expand_dims(source, axis=-1)
-        print('source data shape: ', self.source_data[0].shape, self.source_data[1].shape)
+        # print('source data shape: ', self.source_data[0].shape, self.source_data[1].shape)
                 
 
         # Load some forward model objects
-        fwd_fixed, leadfield = util.unpack_fwd(self.fwd)[:2]
-        n_elec = leadfield.shape[0]
+        n_elec = self.leadfield.shape[0]
         n_samples = np.clip(len(self.source_data), a_min=1, a_max=np.inf).astype(int)
 
         target_snrs = [self.get_from_range(self.settings['target_snr'], dtype=float) for _ in range(n_samples)]
-        betas = [self.get_from_range(self.settings['beta'], dtype=float) for _ in range(n_samples)]
+        betas_noise = [self.get_from_range(self.settings['beta_noise'], dtype=float) for _ in range(n_samples)]
 
         # Document snr and beta into the simulation info
-        
-        self.simulation_info['betas'] = betas
+        self.simulation_info['betas_noise'] = betas_noise
         self.simulation_info['target_snr'] = target_snrs
     
         # Desired Dim for eeg_clean: (samples, electrodes, time points)
         if self.verbose:
             print(f'\nProject sources to EEG...')
         eeg_clean = self.project_sources(self.source_data)
         # print(type(eeg_clean), eeg_clean[0].shape)
         if self.verbose:
             print(f'\nCreate EEG trials with noise...')
         
-        # Parallel processing was removed since it was extraordinarily slow:
-        # if self.parallel:
-        #     eeg_trials_noisy = Parallel(n_jobs=self.n_jobs, backend='loky') \
-        #         (delayed(self.create_eeg_helper)(eeg_clean[sample], n_simulation_trials,
-        #             target_snrs[sample], betas[sample]) 
-        #         for sample in tqdm(range(n_samples)))
-        # else:
-        # eeg_trials_noisy = np.zeros((eeg_clean.shape[0], n_simulation_trials, *eeg_clean.shape[1:]))
-        
+        # Add noise
+        self.noise_generator = NoiseGenerator(self.info)
+
         eeg_trials_noisy = []
         for sample in tqdm(range(n_samples)):
             eeg_trials_noisy.append( self.create_eeg_helper(eeg_clean[sample], 
-                n_simulation_trials, target_snrs[sample], betas[sample]) 
+                target_snrs[sample], betas_noise[sample]) 
             )
+
         for i, eeg_trial_noisy in enumerate(eeg_trials_noisy):
             if len(eeg_trial_noisy.shape) == 2:
-                eeg_trials_noisy[i] = np.expand_dims(eeg_trial_noisy, axis=-1)
-            if eeg_trial_noisy.shape[1] != n_elec:
+                # print("expanding")
+                eeg_trials_noisy[i] = np.expand_dims(eeg_trial_noisy, axis=0)
+                # print("new_shape: ", eeg_trials_noisy[i].shape)
+                
+            if eeg_trials_noisy[i].shape[1] != n_elec:
+                # print(f"problem because n_elec ({n_elec}) must be {eeg_trial_noisy.shape[1]}")
                 eeg_trials_noisy[i] = np.swapaxes(eeg_trial_noisy, 1, 2)
         
         if self.verbose:
             print(f'\nConvert EEG matrices to a single instance of mne.Epochs...')
         ERP_samples_noisy = [np.mean(eeg_trial_noisy, axis=0) for eeg_trial_noisy in eeg_trials_noisy]
-        epochs = util.eeg_to_Epochs(ERP_samples_noisy, fwd_fixed, info=self.info)
+        epochs = util.eeg_to_Epochs(ERP_samples_noisy, self.fwd_fixed, info=self.info)
 
         return epochs
     
-    def create_eeg_helper(self, eeg_sample, n_simulation_trials, target_snr, beta):
+    def create_eeg_helper(self, eeg_sample, target_snr, beta):
         ''' Helper function for EEG simulation that transforms a clean 
             M/EEG signal to a bunch of noisy trials.
 
         Parameters
         ----------
         eeg_sample : numpy.ndarray
-            data sample with dimension (time_points, electrodes)
-        n_simulation_trials : int
-            The number of trials desired
+            data sample with dimension (electrodes, time_points)
         target_snr : float
             The target signal-to-noise ratio
         beta : float
             The beta exponent of the 1/f**beta noise
 
         '''
         
-        assert len(eeg_sample.shape) == 2, 'Length of eeg_sample must be 2 (time_points, electrodes)'
-        
-        eeg_sample = np.repeat(np.expand_dims(eeg_sample, 0), n_simulation_trials, axis=0)
-        snr = target_snr / np.sqrt(n_simulation_trials)
+        assert len(eeg_sample.shape) == 2, 'Length of eeg_sample must be 2 (electrodes, time_points)'
         
         # Before: Add noise based on the GFP of all channels
-        # noise_trial = self.add_noise(eeg_sample, snr, beta=beta)
+        # noise_trial = self.add_noise(eeg_sample, target_snr, beta=beta)
         
         # NEW: ADD noise for different types of channels, separately
         # since they can have entirely different scales.
         coil_types = [ch['coil_type'] for ch in self.info['chs']]
         coil_types_set = list(set(coil_types))
         if len(coil_types_set)>1:
             msg = f'Simulations attempted with more than one channel type \
@@ -530,17 +551,18 @@
         )
         noise_trial = np.zeros(
             eeg_sample.shape
         )
 
         for i, coil_type in enumerate(coil_types_set):
             channel_indices = np.where(coil_type_assignments==i)[0]
-            eeg_sample_temp = eeg_sample[:, channel_indices, :]
-            noise_trial_subtype = self.add_noise(eeg_sample_temp, snr, beta=beta)
-            noise_trial[:, channel_indices, :] = noise_trial_subtype
+            # print(eeg_sample.shape)
+            eeg_sample_temp = eeg_sample[channel_indices, :]
+            noise_trial_subtype = self.add_noise(eeg_sample_temp, target_snr, beta=beta)
+            noise_trial[channel_indices, :] = noise_trial_subtype
 
 
         
 
         return noise_trial
     
     def project_sources(self, sources):
@@ -550,27 +572,26 @@
         sources : numpy.ndarray
             3D array of shape (samples, dipoles, time points)
         
         Return
         ------
 
         '''
-        fwd_fixed, leadfield = util.unpack_fwd(self.fwd)[:2]
         n_samples = len(sources)
-        # n_elec, n_dipoles = leadfield.shape
+        # n_elec, n_dipoles = self.leadfield.shape
         # eeg = np.zeros((n_samples, n_elec, n_timepoints))
         eeg = []
         # Swap axes to dipoles, samples, time_points
         # sources_tmp = np.swapaxes(sources, 0,1)
         # Collapse last two dims into one
         # short_shape = (sources_tmp.shape[0], 
             # sources_tmp.shape[1]*sources_tmp.shape[2])
         # sources_tmp = sources_tmp.reshape(short_shape)
 
-        result = [np.matmul(leadfield, src.data) for src in sources]
+        result = [np.matmul(self.leadfield, src.data) for src in sources]
         
         # Reshape result
         # result = result.reshape(result.shape[0], n_samples, n_timepoints)
         # swap axes to correct order
         # result = np.swapaxes(result,0,1)
         # Rescale
         # result /= scaler
@@ -578,31 +599,34 @@
 
 
     
     def add_noise(self, x, snr, beta=0):
         """ Add noise of given SNR to signal x.
         Parameters:
         -----------
-        x : numpy.ndarray, 3-dimensional numpy array of dims (trials, channels, timepoints)
+        x : numpy.ndarray, 2-dimensional numpy array of dims (channels, timepoints)
         Return:
         -------
         """
     
         # This looks inconvenient but we need to make sure that there is no empty dimension for the powerlaw noise function.
-        x_shape = (x.shape[0], x.shape[1], np.clip(x.shape[2], a_min=2, a_max=np.inf).astype(int))
-        noise = cn.powerlaw_psd_gaussian(beta, x_shape)
+        x_shape = (x.shape[0], np.clip(x.shape[1], a_min=2, a_max=np.inf).astype(int))
+        # noise = cn.powerlaw_psd_gaussian(beta, x_shape)
+        # print("old noise: ", noise.shape)
+        noise = self.noise_generator.get_noise(n_time=x_shape[-1], exponent=beta)
+        # print("new noise: ", noise.shape)
         
         # In case we added another entry in the 2nd dimension we have to remove it here again.
-        if x_shape[2] != x.shape[2]:
-            noise=noise[:, :, :1]
+        if x_shape[1] != x.shape[1]:
+            noise=noise[:, :1]
     
-        noise_gfp = np.std(noise, axis=1)
+        noise_gfp = np.std(noise, axis=0)
         rms_noise = np.median(noise_gfp)  # rms(noise)
         
-        x_gfp = np.std(x, axis=1)
+        x_gfp = np.std(x, axis=0)
         rms_x = np.median(x_gfp)  # np.mean(np.max(np.abs(x_gfp), axis=1))  # x.max()
         if rms_x == 0:  
             # in case most of the signal is zero, e.g. when using biphasic pulses
             rms_x = abs(x_gfp).max()
         # rms_noise = rms(noise-np.mean(noise))
         noise_scaler = rms_x / (rms_noise*snr)
         # print(f'rms_x = {rms_x}\nrms_noise = {rms_noise}\n\tScaling by {noise_scaler} to yield snr of {snr}')
@@ -612,16 +636,15 @@
 
     def check_settings(self):
         ''' Check if settings are complete and insert missing 
             entries if there are any.
         '''
         if self.settings is None:
             self.settings = DEFAULT_SETTINGS
-        
-        _, _, self.pos, _ = util.unpack_fwd(self.fwd)
+        self.fwd_fixed, self.leadfield, self.pos, _ = util.unpack_fwd(self.fwd)
         self.distance_matrix = cdist(self.pos, self.pos)
 
         # Check for wrong keys:
         for key in self.settings.keys():
             if not key in DEFAULT_SETTINGS.keys():
                 msg = f'key {key} is not part of allowed settings. See DEFAULT_SETTINGS for reference: {DEFAULT_SETTINGS}'
                 raise AttributeError(msg)
@@ -636,16 +659,14 @@
             self.temporal = False
         else:
             self.temporal = True
         
         self.neighbors = self.calculate_neighbors()
         
 
-            
-
     def calculate_neighbors(self):
         adj = mne.spatial_src_adjacency(self.fwd["src"], verbose=0).toarray().astype(int)
         neighbors = np.array([np.where(a)[0] for a in adj], dtype=object)
         return neighbors
 
                
     @staticmethod
@@ -869,15 +890,15 @@
     def get_diams_per_order(self):
         ''' Calculate the estimated source diameter per neighborhood order.
         '''
         diams = []
         diam = 0
         order = 0
         while diam<100:
-            diam = util.get_source_diam_from_order(order, self.fwd, dists=deepcopy(self.distance_matrix))
+            diam = util.get_source_diam_from_order(order, self.pos, dists=deepcopy(self.distance_matrix))
             diams.append( diam )
             order += 1
         self.diams = np.array(diams)
     
     
 def get_n_order_indices(order, pick_idx, neighbors):
     ''' Iteratively performs region growing by selecting neighbors of 
@@ -894,7 +915,68 @@
         # current_indices = list(np.array( current_indices ).flatten())
         new_indices = [neighbors[i] for i in current_indices]
         new_indices = flatten( new_indices )
         current_indices.extend(new_indices)
         
         current_indices = list(set(current_indices))
     return current_indices
+
+
+class NoiseGenerator:
+    ''' Generates multidimensional colored noise.
+    Parameters
+    ----------
+    info : mne.Info
+        The mne-python Info object, e.g. present in evoked.info
+    '''
+
+    def __init__(self, info):
+        '''
+        Parameters
+        ----------
+        info : mne.Info
+            The mne-python Info object, e.g. present in evoked.info
+        '''
+        self.info = info
+        self.prepare()
+        pass
+    def prepare(self, resolution=16, k_neighbors=5):
+        ''' Prepare the regularly spaced grid.
+        '''
+        
+        # n_time = sim.eeg_data[0].average().times.size
+        # shape = (resolution, resolution, n_time)
+        self.elec_pos = _find_topomap_coords(self.info, self.info.ch_names, ignore_overlap=True)
+        x = np.linspace(self.elec_pos[:, 0].min(), self.elec_pos[:, 0].max(), num=resolution)
+        y = np.linspace(self.elec_pos[:, 1].min(), self.elec_pos[:, 1].max(), num=resolution)
+
+        grid = np.stack(np.meshgrid(x,y, indexing='ij'), axis=0)
+        grid_flat = grid.reshape(2, resolution**2)
+
+        # grid_flat = grid.reshape(grid.shape[0], np.prod(grid.shape[1:])).T
+        neighbor_indices = np.stack([
+            np.argsort(np.sqrt(np.sum((grid_flat.T - coords)**2, axis=1)))[:k_neighbors] for coords in self.elec_pos
+        ], axis=0)
+
+        self.resolution = resolution
+        self.grid = grid
+        self.grid_flat = grid_flat
+        self.k_neighbors = k_neighbors
+        self.neighbor_indices = neighbor_indices
+        
+
+    def get_noise(self, n_time, exponent=2):
+        ''' Create colored noise of spectrum 1/f**exponent. The noise is first
+        generated on an equally spaced grid and transferred the the electrodes
+        using nearest-neighbor interpolation.
+
+        '''
+
+        noise_grid = util.create_n_dim_noise((self.resolution, self.resolution, n_time), exponent=exponent)
+        noise_elec = np.zeros((self.elec_pos.shape[0], n_time))
+        for e, e_pos in enumerate(self.elec_pos):
+            for t in range(n_time):
+                # neighbor_idc = np.argsort(np.sum((self.grid_flat.T - e_pos)**2, axis=1))[:self.k_neighbors]
+                neighbor_idc = self.neighbor_indices[e,:]
+                noise_transformed = np.mean(noise_grid[:, :, t].flatten()[neighbor_idc])
+                noise_elec[e, t] = noise_transformed
+        return noise_elec
```

## esinet/evaluate/evaluate.py

```diff
@@ -63,14 +63,30 @@
     mask : numpy.ndarray
         The source mask
     pos : numpy.ndarray
         The dipole position matrix
     '''
     return pos[np.where(mask==1)[0]]
 
+def eval_residual_variance(M_true, M_est):
+    ''' Calculate the Residual Variance (1- goodness of fit) between the
+    estimated EEG and the original EEG.
+    
+    Parameters
+    ----------
+    M_true : numpy.ndarray
+        The true EEG data (as recorded). May be a single time point or
+        spatio-temporal.
+    M_est : numpy.ndarray
+        The estimated EEG data (projected from the estimated source). May be a
+        single time point or spatio-temporal.
+    '''
+    return 100 *  np.sum( (M_true-M_est)**2 ) / np.sum(M_true**2)
+
+
 def eval_mean_localization_error(y_true, y_est, pos, k_neighbors=5, 
     min_dist=30, threshold=0.1, ghost_thresh=40, argsorted_distance_matrix=None):
     ''' Calculate the mean localization error for an arbitrary number of 
     sources.
     
     Parameters
     ----------
@@ -218,14 +234,15 @@
     # Filter ghost sources
     found_sources = closest_matches[closest_matches<ghost_thresh]
     n_found_sources = len(found_sources)
 
     return n_found_sources
 
 
+
 def eval_mse(y_true, y_est):
     '''Returns the mean squared error between predicted and true source. '''
     return np.mean((y_true-y_est)**2)
 
 def eval_nmse(y_true, y_est):
     '''Returns the normalized mean squared error between predicted and true 
     source.'''
```

## esinet/tests/test_net.py

```diff
@@ -5,15 +5,15 @@
 
 # Crate forward model
 info = forward.get_info(sfreq=100)
 sampling = 'ico3'
 fwd = forward.create_forward_model(sampling=sampling)
 
 @pytest.mark.parametrize("duration_of_trial", [0.0, 0.1, (0.0, 0.1)])
-@pytest.mark.parametrize("model_type", ['lstm', 'fc', 'convdip'])
+@pytest.mark.parametrize("model_type", ['lstm', 'fc', 'cnn', 'convdip'])
 def test_net(duration_of_trial,model_type):
     settings = dict(duration_of_trial=duration_of_trial)
     sim = simulation.Simulation(fwd, info, settings=settings)
     sim.simulate(n_samples=2)
 
     # Create and train net
     net = Net(fwd, n_dense_units=1, n_dense_layers=1, n_lstm_layers=1, model_type=model_type)
```

## esinet/tests/test_simulation.py

```diff
@@ -20,15 +20,15 @@
 @pytest.mark.parametrize("duration_of_trial", [0, 0.1, (0, 0.1)])
 @pytest.mark.parametrize("sample_frequency", [100,])
 @pytest.mark.parametrize("target_snr", [5, ])
 @pytest.mark.parametrize("beta", [1, ])
 @pytest.mark.parametrize("method", ['standard', 'noise', 'mixed'])
 @pytest.mark.parametrize("source_spread", ['region_growing', 'spherical', 'mixed'])
 @pytest.mark.parametrize("source_number_weighting", [True, False])
-@pytest.mark.parametrize("parallel", [True, False])
+@pytest.mark.parametrize("parallel", [False])
 def test_simulation( number_of_sources, extents, amplitudes,
         shapes, duration_of_trial, sample_frequency, target_snr, beta,
         method, source_spread, source_number_weighting, parallel):
 
     settings = {
             'number_of_sources': number_of_sources,
             'extents': extents,
```

## esinet/util/util.py

```diff
@@ -154,33 +154,39 @@
     leadfield : numpy.ndarray
         The leadfield (gain matrix)
     pos : numpy.ndarray
         The positions of dipoles in the source model
     tris : numpy.ndarray
         The triangles that describe the source mmodel
     """
-    if fwd['surf_ori']:
-        fwd_fixed = fwd
-    else:
-        fwd_fixed = mne.convert_forward_solution(fwd, surf_ori=True, force_fixed=True,
-                                                    use_cps=True, verbose=0)
+    fwd_fixed = fwd
+    if not fwd['surf_ori']:
+        print("Forward model does not contain fixed orientations - expect unexpected behavior!")
+        # fwd_fixed = mne.convert_forward_solution(fwd, surf_ori=True, force_fixed=True,
+        #                                             use_cps=True, verbose=0)
+
+    
     tris = fwd['src'][0]['use_tris']
     leadfield = fwd_fixed['sol']['data']
 
     source = fwd['src']
-    try:
-        subject_his_id = source[0]['subject_his_id']
-        pos_left = mne.vertex_to_mni(source[0]['vertno'], 0, subject_his_id, verbose=0)
-        pos_right = mne.vertex_to_mni(source[1]['vertno'],  1, subject_his_id, verbose=0)
-    except:
-        subject_his_id = 'fsaverage'
-        pos_left = mne.vertex_to_mni(source[0]['vertno'], 0, subject_his_id, verbose=0)
-        pos_right = mne.vertex_to_mni(source[1]['vertno'],  1, subject_his_id, verbose=0)
 
-    pos = np.concatenate([pos_left, pos_right], axis=0)
+    if source[0]["type"] == "vol":
+        pos = source[0]["rr"]
+    else:            
+        try:
+            subject_his_id = source[0]['subject_his_id']
+            pos_left = mne.vertex_to_mni(source[0]['vertno'], 0, subject_his_id, verbose=0)
+            pos_right = mne.vertex_to_mni(source[1]['vertno'],  1, subject_his_id, verbose=0)
+        except:
+            subject_his_id = 'fsaverage'
+            pos_left = mne.vertex_to_mni(source[0]['vertno'], 0, subject_his_id, verbose=0)
+            pos_right = mne.vertex_to_mni(source[1]['vertno'],  1, subject_his_id, verbose=0)
+
+        pos = np.concatenate([pos_left, pos_right], axis=0)
 
     return fwd_fixed, leadfield, pos, tris#
 
 
 def calc_snr_range(mne_obj, baseline_span=(-0.2, 0.0), data_span=(0.0, 0.5)):
     """ Calculate the signal to noise ratio (SNR) range of your mne object.
     
@@ -290,15 +296,15 @@
                 # Remove self-index (otherwise neighbors[idx] is its own neighbor)
                 neighbors[idx] = list(filter(lambda a: a != idx, neighbors[idx]))
             # Remove duplicates
             neighbors[idx] = list(np.unique(neighbors[idx]))
     return neighbors
 
 
-def calculate_source(data_obj, fwd, baseline_span=(-0.2, 0.0), 
+def calculate_source(data_obj, fwd, duration_of_trial=None, baseline_span=(-0.2, 0.0), 
     data_span=(0, 0.5), n_samples=int(1e4), optimizer=None, learning_rate=0.001, 
     validation_split=0.1, n_epochs=100, metrics=None, device=None, delta=1, 
     batch_size=8, loss=None, false_positive_penalty=2, parallel=False, 
     verbose=False):
     ''' The all-in-one convenience function for esinet.
     
     Parameters
@@ -352,16 +358,18 @@
 
     source_estimate = calculate_source(epochs, fwd)
     source_estimate.plot()
 
     '''
     # Calculate signal to noise ratio of the data
     target_snr = calc_snr_range(data_obj, baseline_span=baseline_span, data_span=data_span)
+    if duration_of_trial is None:
+        duration_of_trial = data_obj.times[-1] - data_obj.times[0]
     # Simulate sample M/EEG data
-    sim = simulation.Simulation(fwd, data_obj.info, settings=dict(duration_of_trial=0, target_snr=target_snr), parallel=parallel, verbose=True)
+    sim = simulation.Simulation(fwd, data_obj.info, settings=dict(duration_of_trial=duration_of_trial, target_snr=target_snr), parallel=parallel, verbose=True)
     sim.simulate(n_samples=n_samples)
     # Train neural network
     neural_net = net.Net(fwd, verbose=verbose)
     if int(verbose) > 0:
         neural_net.summary()
         
     neural_net.fit(sim, optimizer=optimizer, learning_rate=learning_rate, 
@@ -369,27 +377,27 @@
         device=device, delta=delta, batch_size=batch_size, loss=loss,
         false_positive_penalty=false_positive_penalty)
     # Predict sources
     source_estimate = neural_net.predict(data_obj)
 
     return source_estimate
 
-def get_source_diam_from_order(order, fwd, dists=None):
+def get_source_diam_from_order(order, pos, dists=None):
     ''' Calculate the estimated source diameter given the neighborhood order.
      Useful to calculate source extents using the region_growing method in the
      esinet.Simulation object.
 
     Parameters
     ----------
     order : int,
         The neighborhood order of interest
-    fwd : mne.Forward
-        The forward model.
+    pos : numpy.ndarray
+        Positions of the source model dipoles.
     '''
-    pos = unpack_fwd(fwd)[2]
+    # pos = unpack_fwd(fwd)[2]
     if dists is None:
         dists = cdist(pos, pos)
     dists[dists==0] = np.nan
     return np.median(np.nanmin(dists, axis=0))*(2+order)
 
 def get_eeg_from_source(stc, fwd, info, tmin=-0.2):
     ''' Get EEG from source by projecting source activity through the lead field.
```

## Comparing `esinet-0.2.5.dist-info/LICENSE` & `esinet-0.3.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `esinet-0.2.5.dist-info/METADATA` & `esinet-0.3.0.dist-info/METADATA`

 * *Files 16% similar despite different names*

```diff
@@ -1,138 +1,138 @@
-Metadata-Version: 2.1
-Name: esinet
-Version: 0.2.5
-Summary: Solves the M/EEG inverse problem using artificial neural networks with Python 3 and the MNE library.
-Home-page: https://github.com/LukeTheHecker/esinet
-Author: Lukas Hecker
-Author-email: lukas_hecker@web.de
-Classifier: Programming Language :: Python :: 3
-Classifier: License :: OSI Approved :: MIT License
-Classifier: Operating System :: OS Independent
-Requires-Python: >=3.8.3
-Description-Content-Type: text/markdown
-License-File: LICENSE
-Requires-Dist: tensorflow (>=2.4.1)
-Requires-Dist: mne (>=0.22.0)
-Requires-Dist: scipy
-Requires-Dist: colorednoise
-Requires-Dist: joblib
-Requires-Dist: matplotlib
-Requires-Dist: pyvista
-Requires-Dist: pyvistaqt
-Requires-Dist: vtk
-Requires-Dist: tqdm
-Requires-Dist: pytest
-Requires-Dist: dill
-Requires-Dist: scikit-learn
-Requires-Dist: pandas
-
-# esinet: Electric Source Imaging using Artificial Neural Networks (ANNs)
-
-**esinet** let's you solve the EEG inverse problem using ANNs with the
-mne-python framework. It currently supports three main architectures:
-
-
-## ConvDip
-A convolutional neural network as described in our [first paper](https://www.frontiersin.org/articles/10.3389/fnins.2021.569918/full).
-
-
-## Fully-connected network
-
-A fully-connected neural network which is trained on single time instances of M/EEG data. This model was described in our [second paper](https://www.biorxiv.org/content/10.1101/2022.04.13.488148v1) alongside the LSTM.
-
-## Long-short term memory network
-A Long-short-term memory (LSTM) model which is trained on sequences of EEG data. This model is described in detail and published in our [second paper](https://www.biorxiv.org/content/10.1101/2022.04.13.488148v1).
-
----
-
-<!-- ![esinet](/assets/esinet.png) -->
-<img src="assets/esinet.png" alt="esinet" width="500"/>
-
-Neural network design was created [here](http://alexlenail.me/NN-SVG/index.html)
-
-<br/><br/>
-
-## Dependencies:
-* [Python >= 3.8.3](https://www.python.org/downloads/)
-* [mne](https://mne.tools/stable/index.html)
-  * Follow the [installation guide](https://mne.tools/stable/install/mne_python.html#installing-mne-python-and-its-dependencies)
-* [Tensorflow>=2.4.1](https://www.tensorflow.org/)
-  * Follow the [installation guide](https://www.tensorflow.org/install)
-* [Colorednoise](https://github.com/felixpatzelt/colorednoise)
-* [joblib](https://joblib.readthedocs.io/en/latest/#)
-* [pyvista>=0.24](https://docs.pyvista.org/)
-* [pyvistaqt>=0.2.0](https://qtdocs.pyvista.org/)
-* [tqdm](https://github.com/tqdm/tqdm)
-* [dill](https://dill.readthedocs.io/en/latest/dill.html)
-* [sklearn](https://scikit-learn.org/stable/)
-
-<br/>
-
-# Installation from PyPi
-Use [pip](https://pip.pypa.io/en/stable/) to install esinet and all its
-dependencies from [PyPi](https://pypi.org/):
-
-```
-pip install esinet
-```
-
-<br/>
-
-# Quick start
-The following code demonstrates how to use this package:
-
-```
-from esinet import Simulation, Net
-from esinet.forward import create_forward_model, get_info
-
-# Create generic Forward Model
-info = get_info()
-fwd = create_forward_model(info=info, sampling='ico2')
-
-# Simulate M/EEG data
-settings = dict(duration_of_trial=0.1)
-sim = Simulation(fwd, info, settings=settings)
-sim.simulate(n_samples=200)
-
-# Train neural network (LSTM) on the simulated data
-net = Net(fwd)
-net.fit(sim)
-
-# Plot
-stc = net.predict(sim)[0]
-sim.source_data[0].plot()
-stc.plot()
-```
-
-# First steps
-
-Check out one of the [tutorials](tutorials/) to learn how to use the package:
-
-* [Tutorial 1](tutorials/tutorial_1.ipynb): The fastest way to get started with
-  *esinet*. This tutorial can be used as an entry point. If you want to dig
-  deeper you should have a look at the next tutorials, too!
-* [Tutorial 2](tutorials/tutorial_2.ipynb): Use *esinet* with low-level
-  functions that allow for more control over your parameters with respect to
-  simulations and training of the neural network.
-* [Tutorial 3](tutorials/tutorial_3.ipynb): A demonstration of simulation
-  parameters and how they affect the model performance.
-* [opm_source](tutorials/opm_source.ipynb): Source imaging of optically pumped
-  magnetometer (OPM) data. The tutorial is based on the one provided by
-  [mne](https://mne.tools/stable/auto_examples/datasets/opm_data.html#sphx-glr-auto-examples-datasets-opm-data-py)
-
-# Feedback
-Please leave your feedback and bug reports at lukas_hecker@web.de.
-
-<br/>
-
-# Literature
-Please cite these publications if you are using this code:
-
-Hecker, L., Rupprecht, R., van Elst, L. T., & Kornmeier, J. (2022). Long-Short Term Memory Networks for Electric Source Imaging with Distributed Dipole Models. bioRxiv.
-
-Hecker, L., Rupprecht, R., Tebartz van Elst, L., & Kornmeier, J. (2021). ConvDip: A convolutional neural network for better EEG Source Imaging. Frontiers in Neuroscience, 15, 533.
-
-
-
-# Troubleshooting
-* Having problems with the installation? Check the [package requirements](requirements.txt)
+Metadata-Version: 2.1
+Name: esinet
+Version: 0.3.0
+Summary: Solves the M/EEG inverse problem using artificial neural networks with Python 3 and the MNE library.
+Home-page: https://github.com/LukeTheHecker/esinet
+Author: Lukas Hecker
+Author-email: lukas_hecker@web.de
+Classifier: Programming Language :: Python :: 3
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Operating System :: OS Independent
+Requires-Python: >=3.6
+Description-Content-Type: text/markdown
+License-File: LICENSE
+Requires-Dist: tensorflow
+Requires-Dist: mne >=1.7.0
+Requires-Dist: scipy
+Requires-Dist: colorednoise
+Requires-Dist: joblib
+Requires-Dist: matplotlib
+Requires-Dist: pyvista
+Requires-Dist: pyvistaqt
+Requires-Dist: vtk
+Requires-Dist: tqdm
+Requires-Dist: pytest
+Requires-Dist: dill
+Requires-Dist: scikit-learn
+Requires-Dist: pandas
+
+# esinet: Electric Source Imaging using Artificial Neural Networks (ANNs)
+
+**esinet** let's you solve the EEG inverse problem using ANNs with the
+mne-python framework. It currently supports three main architectures:
+
+
+## ConvDip
+A convolutional neural network as described in our [first paper](https://www.frontiersin.org/articles/10.3389/fnins.2021.569918/full).
+
+
+## Fully-connected network
+
+A fully-connected neural network which is trained on single time instances of M/EEG data. This model was described in our [second paper](https://www.biorxiv.org/content/10.1101/2022.04.13.488148v1) alongside the LSTM.
+
+## Long-short term memory network
+A Long-short-term memory (LSTM) model which is trained on sequences of EEG data. This model is described in detail and published in our [second paper](https://www.biorxiv.org/content/10.1101/2022.04.13.488148v1).
+
+---
+
+<!-- ![esinet](/assets/esinet.png) -->
+<img src="assets/esinet.png" alt="esinet" width="500"/>
+
+Neural network design was created [here](http://alexlenail.me/NN-SVG/index.html)
+
+<br/><br/>
+
+## Dependencies:
+* [Python >= 3.8.3](https://www.python.org/downloads/)
+* [mne](https://mne.tools/stable/index.html)
+  * Follow the [installation guide](https://mne.tools/stable/install/mne_python.html#installing-mne-python-and-its-dependencies)
+* [Tensorflow>=2.4.1](https://www.tensorflow.org/)
+  * Follow the [installation guide](https://www.tensorflow.org/install)
+* [Colorednoise](https://github.com/felixpatzelt/colorednoise)
+* [joblib](https://joblib.readthedocs.io/en/latest/#)
+* [pyvista>=0.24](https://docs.pyvista.org/)
+* [pyvistaqt>=0.2.0](https://qtdocs.pyvista.org/)
+* [tqdm](https://github.com/tqdm/tqdm)
+* [dill](https://dill.readthedocs.io/en/latest/dill.html)
+* [sklearn](https://scikit-learn.org/stable/)
+
+<br/>
+
+# Installation from PyPi
+Use [pip](https://pip.pypa.io/en/stable/) to install esinet and all its
+dependencies from [PyPi](https://pypi.org/):
+
+```
+pip install esinet
+```
+
+<br/>
+
+# Quick start
+The following code demonstrates how to use this package:
+
+```
+from esinet import Simulation, Net
+from esinet.forward import create_forward_model, get_info
+
+# Create generic Forward Model
+info = get_info()
+fwd = create_forward_model(info=info, sampling='ico2')
+
+# Simulate M/EEG data
+settings = dict(duration_of_trial=0.1)
+sim = Simulation(fwd, info, settings=settings)
+sim.simulate(n_samples=200)
+
+# Train neural network (LSTM) on the simulated data
+net = Net(fwd)
+net.fit(sim)
+
+# Plot
+stc = net.predict(sim)[0]
+sim.source_data[0].plot()
+stc.plot()
+```
+
+# First steps
+
+Check out one of the [tutorials](tutorials/) to learn how to use the package:
+
+* [Tutorial 1](tutorials/tutorial_1.ipynb): The fastest way to get started with
+  *esinet*. This tutorial can be used as an entry point. If you want to dig
+  deeper you should have a look at the next tutorials, too!
+* [Tutorial 2](tutorials/tutorial_2.ipynb): Use *esinet* with low-level
+  functions that allow for more control over your parameters with respect to
+  simulations and training of the neural network.
+* [Tutorial 3](tutorials/tutorial_3.ipynb): A demonstration of simulation
+  parameters and how they affect the model performance.
+* [opm_source](tutorials/opm_source.ipynb): Source imaging of optically pumped
+  magnetometer (OPM) data. The tutorial is based on the one provided by
+  [mne](https://mne.tools/stable/auto_examples/datasets/opm_data.html#sphx-glr-auto-examples-datasets-opm-data-py)
+
+# Feedback
+Please leave your feedback and bug reports at lukas_hecker@web.de.
+
+<br/>
+
+# Literature
+Please cite these publications if you are using this code:
+
+Hecker, L., Rupprecht, R., van Elst, L. T., & Kornmeier, J. (2022). Long-Short Term Memory Networks for Electric Source Imaging with Distributed Dipole Models. bioRxiv.
+
+Hecker, L., Rupprecht, R., Tebartz van Elst, L., & Kornmeier, J. (2021). ConvDip: A convolutional neural network for better EEG Source Imaging. Frontiers in Neuroscience, 15, 533.
+
+
+
+# Troubleshooting
+* Having problems with the installation? Check the [package requirements](requirements.txt)
```

## Comparing `esinet-0.2.5.dist-info/RECORD` & `esinet-0.3.0.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-esinet/__init__.py,sha256=WUEeRoULY-FcrNLOC3q2v2q6ZO5O0f5DxwkzncYH9Mc,56
+esinet/__init__.py,sha256=tgVo11pARQ0RgTDDkhpIy_AL-LaL6zBRL5PGegZbDP4,64
 esinet/custom_layers.py,sha256=-eSyeCVhfAyIOZLP9PJ1V8J9tycOen-qbetWxuH96qk,3969
-esinet/losses.py,sha256=0tIaeXSQxmLKsnVQ1Q39fgg5NmC-L4cKI1MDlDP1qOU,8434
-esinet/net.py,sha256=EAoepfKYpvxwtpHs2Nzf0Z3JqA9hdu7ZKIfOR9tBNxo,60610
-esinet/simulation.py,sha256=ulkhVn4O6Jxc-yHs2nziEQ9QQzaVhA8P2NPbsM5BZbI,36168
+esinet/losses.py,sha256=1qxMgx3eTHI1EilOV-pAA8KS17KyNgtWCBbzXMUYBAo,10201
+esinet/net.py,sha256=6Xku6eF1ag5LxwfKnxKFtH-Z-8Yyn-IrNSVXigxNG88,56596
+esinet/simulation.py,sha256=nKCQaA4GjipteXAAGyZDIzwZukKrd3AH8lsVyv-33fU,39299
 esinet/evaluate/__init__.py,sha256=L5i9F32DSAtDeOuL7vexCKHeYsNy03ewuIcIIDkJxRY,23
-esinet/evaluate/evaluate.py,sha256=UWqB2Fy_JM7PNhQwiViQEPDIUP0AWiVYxhy69-wTq6k,18892
+esinet/evaluate/evaluate.py,sha256=9T3_SyR6EvsdAUL-M1u32V93fYo78q6evIuAoRzcP4Y,19451
 esinet/forward/__init__.py,sha256=Rl7hjYh77NUlXdrbdd7BEf5asXWs6Nyz-NVxSNocoJI,22
 esinet/forward/forward.py,sha256=Os2jQmr6P8-4GYxt5Mix1VeJVZX4PUnSSg_gBLFgWTo,3028
 esinet/minimum_norm/__init__.py,sha256=zjviRQpJNLXjmRqVWIwYpUYDxz8KVXer9nm4WvAg_2U,27
 esinet/minimum_norm/minimum_norm.py,sha256=_DR2dfMwNp4A54qdLZCqBYrpd7et-CHYj4KIX_zv-5M,2390
 esinet/tests/__init__.py,sha256=DbiCEGF8BtUgAQzumQjMCub44IPtnvzF0KkQEsodIB8,32
-esinet/tests/test_net.py,sha256=xtjDSstFe9-6t_29whc43cxSYfSl0iZrKInDpAyrgBY,1386
-esinet/tests/test_simulation.py,sha256=g7UAJUV4SYnpmXO5zw5ik0kZDWvUq-DAgJCujYy_JRo,2384
+esinet/tests/test_net.py,sha256=BjktCCCx3G-cqgRIFYSVbYmgueHsy1aYEimJQTZy6no,1393
+esinet/tests/test_simulation.py,sha256=7In_U65p5-T3I4ksqmtZDFW3pPdGu33LH44C8OhhxiE,2378
 esinet/util/__init__.py,sha256=4g0apI8-3diyfdBaBOt6VKY0WGXTK8FpybbNTMHNR2o,19
-esinet/util/util.py,sha256=2iEH8jMYYf0Y2JUGgaY16XZGHQHeiV7eyBMHWGANFMc,27022
+esinet/util/util.py,sha256=hf-itfFxfrIkobMLY_KmVJ-ypjiYB5DVyFItKVrNfkk,27416
 esinet/viz/__init__.py,sha256=yp5ZCxSyFhW8znSt-JKpFAq_K09pFM80h8hI-rZ7nkY,40
 esinet/viz/cmaps.py,sha256=2j5drrfQVsHA86ffZgTXnyID_sDlCCzh48z6Xx43wC4,2898
 esinet/viz/viz.py,sha256=YdcAKsA4gCS1yMdeJgX3GaR5r7AGef_k9ajcpIUgzI4,65
-esinet-0.2.5.dist-info/LICENSE,sha256=iYT8IVW0CGsFDrTP72fcztJiTz0qR7ea6Gwz9eGJa-Q,1088
-esinet-0.2.5.dist-info/METADATA,sha256=ny5sA9FeUgVyPPk9HT1_lRkI9c3F-Jvml8D-APMngzM,4904
-esinet-0.2.5.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-esinet-0.2.5.dist-info/top_level.txt,sha256=9CMZ7wC9oe7IcfHNzdcTC1qNUAcTfn92nuWwMMNMlcU,7
-esinet-0.2.5.dist-info/RECORD,,
+esinet-0.3.0.dist-info/LICENSE,sha256=iYT8IVW0CGsFDrTP72fcztJiTz0qR7ea6Gwz9eGJa-Q,1088
+esinet-0.3.0.dist-info/METADATA,sha256=IzQpHQO5WILJd3eT8hqeSlpiOOr54v8-ivNu3xRjA_o,5027
+esinet-0.3.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+esinet-0.3.0.dist-info/top_level.txt,sha256=9CMZ7wC9oe7IcfHNzdcTC1qNUAcTfn92nuWwMMNMlcU,7
+esinet-0.3.0.dist-info/RECORD,,
```

